{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brettchase/Library/Python/3.9/lib/python/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available Feature Sets: dict_keys(['ma', 'rsi', 'macd', 'volume', 'atr_adx', 'volatility', 'vix_skew', 'experimental_slope', 'past_return'])\n"
     ]
    }
   ],
   "source": [
    "import performance_flow\n",
    "import importlib\n",
    "importlib.reload(performance_flow)\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "import train_flow\n",
    "importlib.reload(train_flow)\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "# --------- #\n",
    "# LEAVE ME  #\n",
    "# --------- #\n",
    "ticker = 'QQQ'\n",
    "include_minute_feats = \"N\"\n",
    "returns = [1, 2, 3, 5, 10, 20, 30]\n",
    "df_daily, feature_sets, return_cols, daily_cols, feature_dict, features = train_flow.import_data(ticker, include_minute_feats, returns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Full Run\n",
    "- Retrain ALL models through most recent aod\n",
    "- Calculate performance\n",
    "- Select and save top n\n",
    "- Make predictions\n",
    "- Ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Return_1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6778</th>\n",
       "      <td>2026-02-19</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6777</th>\n",
       "      <td>2026-02-18</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6776</th>\n",
       "      <td>2026-02-17</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6775</th>\n",
       "      <td>2026-02-13</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6774</th>\n",
       "      <td>2026-02-12</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1999-03-16</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1999-03-15</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1999-03-12</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1999-03-11</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1999-03-10</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6779 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            Date  Return_1\n",
       "6778  2026-02-19         1\n",
       "6777  2026-02-18         0\n",
       "6776  2026-02-17         1\n",
       "6775  2026-02-13         0\n",
       "6774  2026-02-12         1\n",
       "...          ...       ...\n",
       "4     1999-03-16         0\n",
       "3     1999-03-15         1\n",
       "2     1999-03-12         1\n",
       "1     1999-03-11         0\n",
       "0     1999-03-10         1\n",
       "\n",
       "[6779 rows x 2 columns]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_daily[['Date', f'Return_{r}']].dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Horizon 1 Top 3 Models Saved\n",
      "1 | run_separately | ma_sma-volu | 3 | 1.5 | 4 | 1\n",
      "Running new predictions for horizon 1 | run_separately\n",
      "1 | run_separately | volu-atr_adxvola | 3 | 1.5 | 4 | 1\n",
      "Running new predictions for horizon 1 | run_separately\n",
      "1 | run_separately | volu-vix_skew | 3 | 1.5 | 4 | 1\n",
      "Running new predictions for horizon 1 | run_separately\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'pprec'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/pandas/core/indexes/base.py:3812\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3811\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 3812\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3813\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[0;32mpandas/_libs/index.pyx:167\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/index.pyx:196\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7088\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7096\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'pprec'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[38], line 297\u001b[0m\n\u001b[1;32m    294\u001b[0m ensemble_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mensemble_pred\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m (ensemble_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mensemble_edge\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m/\u001b[39m n \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m.5\u001b[39m)\u001b[38;5;241m.\u001b[39mround(\u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m    295\u001b[0m ensemble_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDays_to_Exp\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m ensemble_df\u001b[38;5;241m.\u001b[39mindex \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m--> 297\u001b[0m pos_prec, neg_prec \u001b[38;5;241m=\u001b[39m \u001b[43mensemble_perf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpred_filtered\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    298\u001b[0m ensemble_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpos_prec\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m pos_prec\n\u001b[1;32m    299\u001b[0m ensemble_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mneg_prec\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m neg_prec\n",
      "Cell \u001b[0;32mIn[38], line 86\u001b[0m, in \u001b[0;36mensemble_perf\u001b[0;34m(r, output_df)\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mensemble_perf\u001b[39m(r, output_df):\n\u001b[1;32m     84\u001b[0m     output_df\u001b[38;5;241m.\u001b[39mloc[output_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpred\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0.45\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpred\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m output_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpred\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m---> 86\u001b[0m     output_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpred_edge\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mwhere(output_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpred\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m, \u001b[43moutput_df\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mpprec\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m \u001b[38;5;241m-\u001b[39m output_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpos_rate\u001b[39m\u001b[38;5;124m'\u001b[39m], \u001b[38;5;241m-\u001b[39moutput_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnprec\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m+\u001b[39m(\u001b[38;5;241m1\u001b[39m\u001b[38;5;241m-\u001b[39moutput_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpos_rate\u001b[39m\u001b[38;5;124m'\u001b[39m]))\u001b[38;5;241m.\u001b[39mround(\u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m     88\u001b[0m     ensemble_df \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m     89\u001b[0m         output_df\n\u001b[1;32m     90\u001b[0m         \u001b[38;5;241m.\u001b[39mgroupby(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDate\u001b[39m\u001b[38;5;124m'\u001b[39m, as_index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     99\u001b[0m         \u001b[38;5;241m.\u001b[39mrename(columns\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpred_edge\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mensemble_edge\u001b[39m\u001b[38;5;124m'\u001b[39m})\n\u001b[1;32m    100\u001b[0m     )\n\u001b[1;32m    101\u001b[0m     ensemble_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mensemble_pred\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m (ensemble_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mensemble_edge\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m/\u001b[39m n \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m.5\u001b[39m)\u001b[38;5;241m.\u001b[39mround(\u001b[38;5;241m2\u001b[39m)\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/pandas/core/frame.py:4113\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   4111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   4112\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[0;32m-> 4113\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4114\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[1;32m   4115\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/pandas/core/indexes/base.py:3819\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3814\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[1;32m   3815\u001b[0m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[1;32m   3816\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[1;32m   3817\u001b[0m     ):\n\u001b[1;32m   3818\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[0;32m-> 3819\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01merr\u001b[39;00m\n\u001b[1;32m   3820\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m   3821\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[1;32m   3822\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[1;32m   3823\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[1;32m   3824\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'pprec'"
     ]
    }
   ],
   "source": [
    "from itertools import chain\n",
    "import deployment_flow, performance_flow\n",
    "import importlib\n",
    "importlib.reload(deployment_flow)\n",
    "importlib.reload(performance_flow)\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def resolve_feature_cols(feature_set_name: str, features_dict: dict, sep: str = \"-\") -> list[str]:\n",
    "\n",
    "    # --- Kitchen sink case ---\n",
    "    if feature_set_name == \"kitch_sink\":\n",
    "        all_cols = chain.from_iterable(features_dict.values())\n",
    "        # dedupe preserve order\n",
    "        seen = set()\n",
    "        out = []\n",
    "        for c in all_cols:\n",
    "            if c not in seen:\n",
    "                seen.add(c)\n",
    "                out.append(c)\n",
    "        return out\n",
    "\n",
    "    # --- Normal composite case ---\n",
    "    parts = feature_set_name.split(sep)\n",
    "\n",
    "    cols = []\n",
    "    for p in parts:\n",
    "        if p not in features_dict:\n",
    "            raise KeyError(f\"{p} not in features_dict\")\n",
    "        cols.append(features_dict[p])\n",
    "\n",
    "    # flatten + dedupe\n",
    "    seen = set()\n",
    "    out = []\n",
    "    for c in chain.from_iterable(cols):\n",
    "        if c not in seen:\n",
    "            seen.add(c)\n",
    "            out.append(c)\n",
    "\n",
    "    return out\n",
    "\n",
    "def pred_chart(df):\n",
    "\n",
    "    x = df[\"Days_to_Exp\"].astype(int)\n",
    "    y = df[\"Predicted_Price\"].astype(float)\n",
    "    last_close = float(df[\"Last_Close\"].iloc[0])\n",
    "\n",
    "    # size scaling\n",
    "    dist = np.abs(df[\"ensemble_pred\"] - 0.5)\n",
    "    sizes = 25 + 400 * dist   # adjust 400 to taste\n",
    "\n",
    "    green_mask = df[\"ensemble_pred\"] > 0.55\n",
    "    red_mask   = df[\"ensemble_pred\"] < 0.45\n",
    "    neutral    = ~(green_mask | red_mask)\n",
    "\n",
    "    plt.figure(figsize=(10,6))\n",
    "\n",
    "    # neutral → white fill, black edge\n",
    "    plt.scatter(x[neutral], y[neutral],\n",
    "                s=sizes[neutral],\n",
    "                facecolors=\"white\", edgecolors=\"black\")\n",
    "\n",
    "    # green\n",
    "    plt.scatter(x[green_mask], y[green_mask],\n",
    "                s=sizes[green_mask], color=\"green\")\n",
    "\n",
    "    # red\n",
    "    plt.scatter(x[red_mask], y[red_mask],\n",
    "                s=sizes[red_mask], color=\"red\")\n",
    "\n",
    "\n",
    "    # last close line\n",
    "    plt.hlines(last_close, xmin=x.min(), xmax=x.max(),\n",
    "            colors=\"black\", linewidth=2)\n",
    "\n",
    "    plt.xlabel(\"Days to Exp\")\n",
    "    plt.ylabel(\"Price\")\n",
    "    plt.title(\"Predicted Price vs Days to Exp\")\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.show()\n",
    "  \n",
    "def ensemble_perf(r, output_df):\n",
    "\n",
    "    output_df.loc[output_df['pred'] < 0.45, 'pred'] = output_df['pred'] - 1\n",
    "\n",
    "    output_df['pred_edge'] = np.where(output_df['pred'] > 0, output_df['pprec'] - output_df['pos_rate'], -output_df['nprec'] +(1-output_df['pos_rate'])).round(2)\n",
    "\n",
    "    ensemble_df = (\n",
    "        output_df\n",
    "        .groupby('Date', as_index=False)\n",
    "        .agg({\n",
    "            'Date': 'first',\n",
    "            'horizon': 'first',\n",
    "            'Predicted_Price': 'first',\n",
    "            'Last_Close': 'first',\n",
    "            'LC_R_PP': 'first',\n",
    "            'pred_edge': 'sum'\n",
    "        })\n",
    "        .rename(columns={'pred_edge': 'ensemble_edge'})\n",
    "    )\n",
    "    ensemble_df['ensemble_pred'] = (ensemble_df['ensemble_edge'] / n + .5).round(2)\n",
    "    ensemble_df['Days_to_Exp'] = ensemble_df.index + 1\n",
    "\n",
    "    ensemble_perf_df = ensemble_df.merge(df_daily[['Date', f'Return_{r}']].dropna(), on='Date', how='inner')\n",
    "\n",
    "    ensemble_perf_df['acc'] = np.where(\n",
    "        ((ensemble_perf_df['ensemble_pred'] > 0.5) & (ensemble_perf_df[f'Return_{r}'] == 1)) |\n",
    "        ((ensemble_perf_df['ensemble_pred'] < 0.5) & (ensemble_perf_df[f'Return_{r}'] == 0)),\n",
    "        1,\n",
    "        0\n",
    "    )\n",
    "\n",
    "    df = ensemble_perf_df.copy()\n",
    "\n",
    "    # predicted class\n",
    "    df[\"pred_class\"] = np.where(df[\"ensemble_pred\"] > 0.5, 1,\n",
    "                        np.where(df[\"ensemble_pred\"] < 0.5, 0, np.nan))\n",
    "\n",
    "    # positive precision\n",
    "    pos_mask = df[\"pred_class\"] == 1\n",
    "    pos_prec = (df.loc[pos_mask, f'Return_{r}'] == 1).mean().round(3)\n",
    "\n",
    "    # negative precision\n",
    "    neg_mask = df[\"pred_class\"] == 0\n",
    "    neg_prec = (df.loc[neg_mask, f'Return_{r}'] == 0).mean().round(3)\n",
    "\n",
    "    return pos_prec, neg_prec\n",
    "\n",
    "h=[1, 2, 5, 10, 20, 30]\n",
    "master_results = []\n",
    "master_preds = []\n",
    "n = 3 # number of top models to select \n",
    "file_ext = \"performance_all\"\n",
    "min_th = 0.55\n",
    "cov_th = 0.75\n",
    "perf_cutoff_date = '2025-09-01'\n",
    "ensemble_all_df = pd.DataFrame()\n",
    "ensemble_frames = []\n",
    "\n",
    "# Retrain ALL\n",
    "for r in h:\n",
    "    \n",
    "    df = pd.read_csv(f\"h{r}_{file_ext}.csv\")\n",
    "    df = df.dropna().copy()\n",
    "    #df = df.rename(columns={\"feature_set\": \"features\"})\n",
    "\n",
    "    df[\"feature_cols\"] = df[\"features\"].apply(lambda x: resolve_feature_cols(x, feature_dict))\n",
    "\n",
    "    grain_cols = [\"horizon\",\"features\",\"train_years\",\"min_feats\",\"pi_size\",\"model\",\"pi_handling\"]\n",
    "\n",
    "    max_train = (\n",
    "        df.groupby(grain_cols, as_index=False)[\"test_start\"]\n",
    "        .max()\n",
    "        .rename(columns={\"test_start\": \"max_test_start\"})\n",
    "    )\n",
    "\n",
    "    models = (\n",
    "        df[grain_cols].drop_duplicates(subset=grain_cols, keep=\"first\")\n",
    "        .merge(df[grain_cols + [\"feature_cols\"]].drop_duplicates(subset=grain_cols), on=grain_cols, how=\"left\")\n",
    "        .merge(max_train, on=grain_cols, how=\"left\")   # <-- this is the missing piece\n",
    "    )\n",
    "    \n",
    "    for row in models.itertuples(index=False):\n",
    "\n",
    "        target_horizon = row.horizon\n",
    "        pi_handling    = 'run_separately' #row.pi_handling\n",
    "        type           = 'Actualized'\n",
    "        feature_cols   = row.feature_cols   # list-of-cols wrapped in a list\n",
    "        list_name      = row.features\n",
    "        train_year     = row.train_years\n",
    "        pi_year        = row.pi_size\n",
    "        min_feat       = row.min_feats\n",
    "        max_test_start = row.max_test_start\n",
    "        days_assessed  = len(df_daily.iloc[r:][df_daily['Date'] > max_test_start].copy())\n",
    "        groups = list_name.split(\"-\")\n",
    "\n",
    "        if days_assessed > 0:\n",
    "\n",
    "            model = XGBClassifier(n_estimators=300, random_state=42, n_jobs=-1)\n",
    "            model_name = \"xgboost-3\"\n",
    "\n",
    "            print(f\"{target_horizon} | {pi_handling} | {list_name} | {train_year} | {pi_year} | {min_feat} | {days_assessed}\")\n",
    "            results_df = deployment_flow.run_deploy_flow(days_assessed, r, pi_handling, feature_cols, df_daily, model_name, model,\n",
    "                            train_year, pi_year, min_feat, list_name, feature_dict, groups, type)\n",
    "            \n",
    "            master_results.append(results_df)\n",
    "    \n",
    "    #print(f\"Retrainig Done\")\n",
    "    if len(master_results) > 0: \n",
    "        \n",
    "        master_results_df = pd.concat(master_results, ignore_index=True)\n",
    "        performance_df = pd.read_csv(f\"h{r}_{file_ext}.csv\")\n",
    "        df_concat = pd.concat([performance_df, master_results_df], ignore_index=True)    \n",
    "        df_concat.to_csv(f\"h{r}_{file_ext}.csv\", index=False)\n",
    "\n",
    "# Performance and Top n\n",
    "for r in h:\n",
    "\n",
    "    keys = [\"horizon\", \"features\", \"train_years\", \"min_feats\", \"pi_size\", \"pi_handling\", \"model\"]\n",
    "\n",
    "    results_file_name = f\"h{r}_{file_ext}.csv\" # Match prior cell saved as file name horizon_2_baseline_new\n",
    "    return_cols, perf_df = performance_flow.import_data(results_file_name, df_daily)\n",
    "    perf_df = perf_df[perf_df['test_start'] >= perf_cutoff_date].rename(columns={\"feature_set\": \"features\"})\n",
    "    composite_score = performance_flow.run_performance(perf_df[perf_df['horizon'] == r].dropna(), min_th, cov_th)\n",
    "    bucket_df = performance_flow.bucket_scores(df_daily.dropna(), perf_df[perf_df['horizon'] == r].dropna(), returns, min_th, keys)\n",
    "\n",
    "    top_n = (\n",
    "    composite_score.sort_values(\"composite\", ascending=False)\n",
    "    .drop_duplicates(subset=[\"features\"], keep=\"first\").head(n).copy())\n",
    "\n",
    "    # Ensure dtypes match so the join actually hits\n",
    "    for df in (top_n, perf_df):\n",
    "\n",
    "        df[\"horizon\"] = r\n",
    "        df[\"features\"] = df[\"features\"].astype(str)\n",
    "        df[\"model\"]       = df[\"model\"].astype(str)\n",
    "        df[\"pi_size\"]     = df[\"pi_size\"]\n",
    "        df[\"pi_handling\"]     = df[\"pi_handling\"].astype(str)\n",
    "        df[\"train_years\"] = df[\"train_years\"].astype(int)\n",
    "        df[\"min_feats\"]   = df[\"min_feats\"].astype(int)\n",
    "\n",
    "    # Filter master predictions to only rows matching one of the 10 configs\n",
    "    pred_filtered = perf_df.merge(top_n[keys].drop_duplicates(), on=keys, how=\"inner\")\n",
    "    #print(len(pred_filtered))\n",
    "    pred_filtered.to_csv(f\"h{r}_top{n}_{file_ext}.csv\", index=False)\n",
    "    print(f\"Horizon {r} Top {n} Models Saved\")\n",
    "\n",
    "    master_preds = []\n",
    "    days_assessed = len(df_daily[df_daily[f\"Return_{r}\"].isna()])\n",
    "\n",
    "    df = pd.read_csv(f\"h{r}_top{n}_{file_ext}.csv\")\n",
    "    df[\"feature_cols\"] = df[\"features\"].apply(lambda x: resolve_feature_cols(x, feature_dict))\n",
    "\n",
    "    grain_cols = [\"horizon\",\"features\",\"train_years\",\"min_feats\",\"pi_size\",\"model\",\"pi_handling\"]\n",
    "\n",
    "    top_n = (\n",
    "        df[grain_cols].drop_duplicates(subset=grain_cols, keep=\"first\")\n",
    "        .merge(df[grain_cols + [\"feature_cols\"]].drop_duplicates(subset=grain_cols), on=grain_cols, how=\"left\"))\n",
    "    \n",
    "    for row in top_n.itertuples(index=False):\n",
    "\n",
    "        target_horizon = row.horizon\n",
    "        pi_handling    = 'run_separately' #row.pi_handling\n",
    "        type           = 'New_Predict'\n",
    "        feature_cols   = row.feature_cols   # list-of-cols wrapped in a list\n",
    "        list_name      = row.features\n",
    "        train_year     = row.train_years\n",
    "        pi_year        = row.pi_size\n",
    "        min_feat       = row.min_feats\n",
    "        groups = list_name.split(\"-\")\n",
    "\n",
    "        model = XGBClassifier(n_estimators=300, random_state=42, n_jobs=-1)\n",
    "        model_name = \"xgboost-3\"\n",
    "\n",
    "        print(f\"{target_horizon} | {pi_handling} | {list_name} | {train_year} | {pi_year} | {min_feat} | {days_assessed}\")\n",
    "        results_df = deployment_flow.run_deploy_flow(days_assessed, r, pi_handling, feature_cols, df_daily, model_name, model,\n",
    "                        train_year, pi_year, min_feat, list_name, feature_dict, groups, type)\n",
    "        \n",
    "        master_preds.append(results_df)\n",
    "        #print(f\"Horizon {r} Top {n} Models Predicted\")\n",
    "\n",
    "    master_preds_df = pd.concat(master_preds, ignore_index=True)\n",
    "    predictions_df = master_preds_df.copy()\n",
    "    composite_score[['pprec', 'nprec'] + keys].drop_duplicates().merge(predictions_df, on=keys, how=\"inner\")\n",
    "\n",
    "#predictions_df.sort_values(by='test_start', ascending=False).head(n)\n",
    "    output_df = composite_score[['pos_rate', 'pprec', 'nprec'] + keys].drop_duplicates().merge(predictions_df, on=keys, how=\"inner\")\n",
    "    output_df = output_df.rename(columns={\"test_start\": \"Date\"})\n",
    "    cols = ['Date', 'features', 'horizon', 'pos_rate', 'pred', 'pprec', 'nprec']\n",
    "    output_df = output_df[cols].sort_values(by='Date').copy()\n",
    "    output_df = output_df.merge(df_daily[['Close', 'Date']].round(2), on='Date', how=\"inner\")\n",
    "    output_df = output_df.rename(columns={\"Close\": \"Predicted_Price\"})\n",
    "    last_close = (df_daily.sort_values(\"Date\", ascending=False).iloc[0][\"Close\"].round(2))\n",
    "    output_df['Last_Close'] = last_close\n",
    "    output_df['LC_R_PP'] = round(output_df['Last_Close'] / output_df['Predicted_Price'] - 1, 3)\n",
    "\n",
    "    output_df.loc[output_df['pred'] < 0.45, 'pred'] = output_df['pred'] - 1\n",
    "\n",
    "    output_df['pred_edge'] = np.where(output_df['pred'] > 0, output_df['pprec'] - output_df['pos_rate'], -output_df['nprec'] +(1-output_df['pos_rate'])).round(2)\n",
    "\n",
    "    ensemble_df = (\n",
    "        output_df\n",
    "        .groupby('Date', as_index=False)\n",
    "        .agg({\n",
    "            'Date': 'first',\n",
    "            'horizon': 'first',\n",
    "            'Predicted_Price': 'first',\n",
    "            'Last_Close': 'first',\n",
    "            'LC_R_PP': 'first',\n",
    "            'pred_edge': 'sum'\n",
    "        })\n",
    "        .rename(columns={'pred_edge': 'ensemble_edge'})\n",
    "    )\n",
    "    ensemble_df['ensemble_pred'] = (ensemble_df['ensemble_edge'] / n + .5).round(2)\n",
    "    ensemble_df['Days_to_Exp'] = ensemble_df.index + 1\n",
    "\n",
    "    pos_prec, neg_prec = ensemble_perf(r, pred_filtered)\n",
    "    ensemble_df['pos_prec'] = pos_prec\n",
    "    ensemble_df['neg_prec'] = neg_prec\n",
    "\n",
    "    ensemble_frames.append(ensemble_df)\n",
    "\n",
    "ensemble_all_df = pd.concat(ensemble_frames, ignore_index=True)\n",
    "\n",
    "pred_chart(ensemble_all_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['pprec', 'nprec', 'horizon', 'features', 'train_years', 'min_feats',\n",
       "       'pi_size', 'pi_handling', 'model', 'run', 'test_days', 'pred',\n",
       "       'pred_class', 'test_pos_n', 'acc', 'train_n', 'train_start',\n",
       "       'train_end', 'test_start', 'test_end', 'n_features', 'Date'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "output_df = pred_filtered.copy()\n",
    "output_df.loc[output_df['pred'] < 0.45, 'pred'] = output_df['pred'] - 1\n",
    "\n",
    "composite_score = performance_flow.run_performance(pred_filtered[pred_filtered['horizon'] == r].dropna(), min_th, cov_th)\n",
    "output_df = composite_score[['pos_rate', 'pprec', 'nprec'] + keys].drop_duplicates().merge(output_df, on=keys, how=\"inner\")\n",
    "\n",
    "output_df['pred_edge'] = np.where(output_df['pred'] > 0, output_df['pprec'] - output_df['pos_rate'], -output_df['nprec'] +(1-output_df['pos_rate'])).round(2)\n",
    "\n",
    "ensemble_df = (\n",
    "    output_df\n",
    "    .groupby('Date', as_index=False)\n",
    "    .agg({\n",
    "        'Date': 'first',\n",
    "        'horizon': 'first',\n",
    "        'pred_edge': 'sum'\n",
    "    })\n",
    "    .rename(columns={'pred_edge': 'ensemble_edge'})\n",
    ")\n",
    "ensemble_df['ensemble_pred'] = (ensemble_df['ensemble_edge'] / n + .5).round(2)\n",
    "ensemble_df['Days_to_Exp'] = ensemble_df.index + 1\n",
    "\n",
    "ensemble_perf_df = ensemble_df.merge(df_daily[['Date', f'Return_{r}']].dropna(), on='Date', how='inner')\n",
    "\n",
    "ensemble_perf_df['acc'] = np.where(\n",
    "    ((ensemble_perf_df['ensemble_pred'] > 0.5) & (ensemble_perf_df[f'Return_{r}'] == 1)) |\n",
    "    ((ensemble_perf_df['ensemble_pred'] < 0.5) & (ensemble_perf_df[f'Return_{r}'] == 0)),\n",
    "    1,\n",
    "    0\n",
    ")\n",
    "\n",
    "df = ensemble_perf_df.copy()\n",
    "\n",
    "# predicted class\n",
    "df[\"pred_class\"] = np.where(df[\"ensemble_pred\"] > 0.5, 1,\n",
    "                    np.where(df[\"ensemble_pred\"] < 0.5, 0, np.nan))\n",
    "\n",
    "# positive precision\n",
    "pos_mask = df[\"pred_class\"] == 1\n",
    "pos_prec = (df.loc[pos_mask, f'Return_{r}'] == 1).mean().round(3)\n",
    "\n",
    "# negative precision\n",
    "neg_mask = df[\"pred_class\"] == 0\n",
    "neg_prec = (df.loc[neg_mask, f'Return_{r}'] == 0).mean().round(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>horizon</th>\n",
       "      <th>ensemble_edge</th>\n",
       "      <th>ensemble_pred</th>\n",
       "      <th>Days_to_Exp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2025-09-02</td>\n",
       "      <td>1</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.55</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2025-09-03</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.50</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2025-09-04</td>\n",
       "      <td>1</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.55</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2025-09-05</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.24</td>\n",
       "      <td>0.42</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2025-09-08</td>\n",
       "      <td>1</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.55</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113</th>\n",
       "      <td>2026-02-12</td>\n",
       "      <td>1</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.55</td>\n",
       "      <td>114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114</th>\n",
       "      <td>2026-02-13</td>\n",
       "      <td>1</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.55</td>\n",
       "      <td>115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115</th>\n",
       "      <td>2026-02-17</td>\n",
       "      <td>1</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.55</td>\n",
       "      <td>116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116</th>\n",
       "      <td>2026-02-18</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.12</td>\n",
       "      <td>0.46</td>\n",
       "      <td>117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117</th>\n",
       "      <td>2026-02-19</td>\n",
       "      <td>1</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.55</td>\n",
       "      <td>118</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>118 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           Date  horizon  ensemble_edge  ensemble_pred  Days_to_Exp\n",
       "0    2025-09-02        1           0.15           0.55            1\n",
       "1    2025-09-03        1           0.00           0.50            2\n",
       "2    2025-09-04        1           0.15           0.55            3\n",
       "3    2025-09-05        1          -0.24           0.42            4\n",
       "4    2025-09-08        1           0.15           0.55            5\n",
       "..          ...      ...            ...            ...          ...\n",
       "113  2026-02-12        1           0.15           0.55          114\n",
       "114  2026-02-13        1           0.15           0.55          115\n",
       "115  2026-02-17        1           0.15           0.55          116\n",
       "116  2026-02-18        1          -0.12           0.46          117\n",
       "117  2026-02-19        1           0.15           0.55          118\n",
       "\n",
       "[118 rows x 5 columns]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ensemble_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
