{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 405,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available Feature Sets: dict_keys(['ma', 'rsi', 'macd', 'volume', 'atr_adx', 'volatility', 'vix_skew', 'experimental_slope'])\n"
     ]
    }
   ],
   "source": [
    "import min_features, daily_return\n",
    "import importlib\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.base import clone\n",
    "from sklearn.metrics import (\n",
    "    balanced_accuracy_score,\n",
    "    accuracy_score,\n",
    "    f1_score,\n",
    "    matthews_corrcoef,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    ")\n",
    "from sklearn.inspection import permutation_importance\n",
    "import warnings\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from xgboost import XGBClassifier\n",
    "warnings.filterwarnings(\"ignore\", message=\"y_pred contains classes not in y_true\")\n",
    "warnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "import time\n",
    "\n",
    "importlib.reload(min_features)\n",
    "importlib.reload(daily_return)\n",
    "\n",
    "min_feats = \"N\"\n",
    "returns = [1, 2, 3, 5, 10, 20, 30]\n",
    "\n",
    "if min_feats != 'N':\n",
    "    df_min = min_features.min_features()\n",
    "    df_daily, feature_sets = daily_return.pull_daily('QQQ', returns) \n",
    "\n",
    "    df_main = pd.merge(df_min, df_daily, how='inner', on='Date')\n",
    "    df_main = df_main.sort_values(by='Date', ascending=False)\n",
    "\n",
    "    return_cols = df_main.columns[df_main.columns.str.contains(\"Return_\")].to_list()\n",
    "    daily_cols = [\n",
    "        c for c in df_daily.iloc[:, 1:].columns\n",
    "        if \"return\" not in c.lower()\n",
    "    ]\n",
    "    close_cols = df_min.columns[(df_min.columns.str.contains(\"close_\")) | (df_min.columns.str.contains(\"post_\")) | (df_min.columns.str.contains(\"overnight_\"))].to_list()\n",
    "    min_cols = (\n",
    "        df_min\n",
    "        .loc[:, ~df_min.columns.isin(close_cols)]  # drop close_ columns\n",
    "        .iloc[:, 1:]                               # drop first column\n",
    "        .columns\n",
    "        .to_list()\n",
    "    )\n",
    "else:\n",
    "    df_daily, feature_sets = daily_return.pull_daily('QQQ', returns) \n",
    "    return_cols = df_daily.columns[df_daily.columns.str.contains(\"Return_\")].to_list()\n",
    "    past_return_cols = df_daily.columns[df_daily.columns.str.contains(\"Past_Ret\")].to_list()\n",
    "    daily_cols = [\n",
    "        c for c in df_daily.iloc[:, 1:].columns\n",
    "        if \"return\" not in c.lower()\n",
    "    ]\n",
    "    df_daily[[f\"{c}_sum10\" for c in df_daily.columns if c.startswith(\"Past_Return_\")]] = (df_daily.sort_values(by=\"Date\", ascending=True).filter(like=\"Past_Return_\").rolling(10, min_periods=1).sum())\n",
    "    new_cols = [c for c in df_daily.columns if c.startswith(\"Past_Return%\") or c.endswith(\"sum10\")]\n",
    "    df_main = df_daily[df_daily['Date'] <= '2026-01-21'].copy()\n",
    "\n",
    "#top_models = pd.read_csv(\"top_performers2.csv\")\n",
    "print(f'Available Feature Sets: {feature_sets.keys()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 407,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Close</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Volume</th>\n",
       "      <th>SMA_10</th>\n",
       "      <th>Close_Rel_Max10</th>\n",
       "      <th>Close_Rel_Min10</th>\n",
       "      <th>SMA_25</th>\n",
       "      <th>Close_Rel_Max25</th>\n",
       "      <th>...</th>\n",
       "      <th>Close_slope10</th>\n",
       "      <th>Close_slope25</th>\n",
       "      <th>Close_slope50</th>\n",
       "      <th>Past_Return_1_sum10</th>\n",
       "      <th>Past_Return_2_sum10</th>\n",
       "      <th>Past_Return_3_sum10</th>\n",
       "      <th>Past_Return_5_sum10</th>\n",
       "      <th>Past_Return_10_sum10</th>\n",
       "      <th>Past_Return_20_sum10</th>\n",
       "      <th>Past_Return_30_sum10</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6758</th>\n",
       "      <td>2026-01-21</td>\n",
       "      <td>616.280029</td>\n",
       "      <td>620.419983</td>\n",
       "      <td>607.859985</td>\n",
       "      <td>79837900</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.98</td>\n",
       "      <td>1.02</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.98</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.00200</td>\n",
       "      <td>0.00064</td>\n",
       "      <td>0.00048</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6757</th>\n",
       "      <td>2026-01-20</td>\n",
       "      <td>608.059998</td>\n",
       "      <td>615.059998</td>\n",
       "      <td>607.049988</td>\n",
       "      <td>81988900</td>\n",
       "      <td>0.98</td>\n",
       "      <td>0.97</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.98</td>\n",
       "      <td>0.97</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.00168</td>\n",
       "      <td>0.00075</td>\n",
       "      <td>0.00050</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6756</th>\n",
       "      <td>2026-01-16</td>\n",
       "      <td>621.260010</td>\n",
       "      <td>626.080017</td>\n",
       "      <td>618.880005</td>\n",
       "      <td>61058100</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.99</td>\n",
       "      <td>1.01</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.99</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00013</td>\n",
       "      <td>0.00080</td>\n",
       "      <td>0.00049</td>\n",
       "      <td>6.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>9.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6755</th>\n",
       "      <td>2026-01-15</td>\n",
       "      <td>621.780029</td>\n",
       "      <td>630.000000</td>\n",
       "      <td>620.750000</td>\n",
       "      <td>53934900</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.99</td>\n",
       "      <td>1.02</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.99</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00117</td>\n",
       "      <td>0.00064</td>\n",
       "      <td>0.00046</td>\n",
       "      <td>6.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6754</th>\n",
       "      <td>2026-01-14</td>\n",
       "      <td>619.549988</td>\n",
       "      <td>623.450012</td>\n",
       "      <td>614.559998</td>\n",
       "      <td>72598700</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.98</td>\n",
       "      <td>1.02</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.98</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00188</td>\n",
       "      <td>0.00052</td>\n",
       "      <td>0.00037</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1999-03-16</td>\n",
       "      <td>43.867695</td>\n",
       "      <td>44.052457</td>\n",
       "      <td>43.207832</td>\n",
       "      <td>4905800</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1999-03-15</td>\n",
       "      <td>43.498177</td>\n",
       "      <td>43.550966</td>\n",
       "      <td>42.152056</td>\n",
       "      <td>6369000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1999-03-12</td>\n",
       "      <td>42.284012</td>\n",
       "      <td>43.207820</td>\n",
       "      <td>41.940883</td>\n",
       "      <td>8743600</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1999-03-11</td>\n",
       "      <td>43.339802</td>\n",
       "      <td>43.696128</td>\n",
       "      <td>42.495177</td>\n",
       "      <td>9688600</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1999-03-10</td>\n",
       "      <td>43.128639</td>\n",
       "      <td>43.207823</td>\n",
       "      <td>42.468776</td>\n",
       "      <td>5232000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6759 rows × 221 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            Date       Close        High         Low    Volume  SMA_10  \\\n",
       "6758  2026-01-21  616.280029  620.419983  607.859985  79837900    0.99   \n",
       "6757  2026-01-20  608.059998  615.059998  607.049988  81988900    0.98   \n",
       "6756  2026-01-16  621.260010  626.080017  618.880005  61058100    1.00   \n",
       "6755  2026-01-15  621.780029  630.000000  620.750000  53934900    1.00   \n",
       "6754  2026-01-14  619.549988  623.450012  614.559998  72598700    1.00   \n",
       "...          ...         ...         ...         ...       ...     ...   \n",
       "4     1999-03-16   43.867695   44.052457   43.207832   4905800     NaN   \n",
       "3     1999-03-15   43.498177   43.550966   42.152056   6369000     NaN   \n",
       "2     1999-03-12   42.284012   43.207820   41.940883   8743600     NaN   \n",
       "1     1999-03-11   43.339802   43.696128   42.495177   9688600     NaN   \n",
       "0     1999-03-10   43.128639   43.207823   42.468776   5232000     NaN   \n",
       "\n",
       "      Close_Rel_Max10  Close_Rel_Min10  SMA_25  Close_Rel_Max25  ...  \\\n",
       "6758             0.98             1.02    1.00             0.98  ...   \n",
       "6757             0.97             1.00    0.98             0.97  ...   \n",
       "6756             0.99             1.01    1.00             0.99  ...   \n",
       "6755             0.99             1.02    1.00             0.99  ...   \n",
       "6754             0.98             1.02    1.00             0.98  ...   \n",
       "...               ...              ...     ...              ...  ...   \n",
       "4                 NaN              NaN     NaN              NaN  ...   \n",
       "3                 NaN              NaN     NaN              NaN  ...   \n",
       "2                 NaN              NaN     NaN              NaN  ...   \n",
       "1                 NaN              NaN     NaN              NaN  ...   \n",
       "0                 NaN              NaN     NaN              NaN  ...   \n",
       "\n",
       "      Close_slope10  Close_slope25  Close_slope50  Past_Return_1_sum10  \\\n",
       "6758       -0.00200        0.00064        0.00048                  5.0   \n",
       "6757       -0.00168        0.00075        0.00050                  5.0   \n",
       "6756        0.00013        0.00080        0.00049                  6.0   \n",
       "6755        0.00117        0.00064        0.00046                  6.0   \n",
       "6754        0.00188        0.00052        0.00037                  5.0   \n",
       "...             ...            ...            ...                  ...   \n",
       "4               NaN            NaN            NaN                  3.0   \n",
       "3               NaN            NaN            NaN                  2.0   \n",
       "2               NaN            NaN            NaN                  1.0   \n",
       "1               NaN            NaN            NaN                  1.0   \n",
       "0               NaN            NaN            NaN                  0.0   \n",
       "\n",
       "      Past_Return_2_sum10  Past_Return_3_sum10  Past_Return_5_sum10  \\\n",
       "6758                  4.0                  5.0                  6.0   \n",
       "6757                  5.0                  6.0                  7.0   \n",
       "6756                  6.0                  6.0                  7.0   \n",
       "6755                  5.0                  6.0                  7.0   \n",
       "6754                  5.0                  6.0                  6.0   \n",
       "...                   ...                  ...                  ...   \n",
       "4                     2.0                  2.0                  0.0   \n",
       "3                     1.0                  1.0                  0.0   \n",
       "2                     0.0                  0.0                  0.0   \n",
       "1                     0.0                  0.0                  0.0   \n",
       "0                     0.0                  0.0                  0.0   \n",
       "\n",
       "      Past_Return_10_sum10  Past_Return_20_sum10  Past_Return_30_sum10  \n",
       "6758                   7.0                   7.0                   7.0  \n",
       "6757                   8.0                   6.0                   8.0  \n",
       "6756                   9.0                   6.0                   9.0  \n",
       "6755                   9.0                   5.0                  10.0  \n",
       "6754                   9.0                   4.0                  10.0  \n",
       "...                    ...                   ...                   ...  \n",
       "4                      0.0                   0.0                   0.0  \n",
       "3                      0.0                   0.0                   0.0  \n",
       "2                      0.0                   0.0                   0.0  \n",
       "1                      0.0                   0.0                   0.0  \n",
       "0                      0.0                   0.0                   0.0  \n",
       "\n",
       "[6759 rows x 221 columns]"
      ]
     },
     "execution_count": 407,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PI Train: 2022-02-16 → 2026-01-13\n",
      "Ran permutation importance for horizon 2 | Len: 484 | Old: 14 | New: 6\n",
      "['Past_Return%_1', 'Past_Return%_10', 'Past_Return%_2', 'Past_Return%_3', 'Past_Return%_30', 'Past_Return%_5']\n",
      "Run 1/230 | Train: 2022-02-16 → 2026-01-13 | Test: 2026-01-16 → 2026-01-16 | Train_n=980 | Test_n=1 | (PI Years: 2 - Feats: 6)\n",
      "PI Train: 2022-02-15 → 2026-01-12\n",
      "Ran permutation importance for horizon 2 | Len: 484 | Old: 14 | New: 6\n",
      "['Past_Return%_1', 'Past_Return%_10', 'Past_Return%_2', 'Past_Return%_3', 'Past_Return%_30', 'Past_Return%_5']\n",
      "Run 2/230 | Train: 2022-02-15 → 2026-01-12 | Test: 2026-01-15 → 2026-01-15 | Train_n=980 | Test_n=1 | (PI Years: 2 - Feats: 6)\n",
      "PI Train: 2022-02-14 → 2026-01-09\n",
      "Ran permutation importance for horizon 2 | Len: 484 | Old: 14 | New: 6\n",
      "['Past_Return%_1', 'Past_Return%_10', 'Past_Return%_2', 'Past_Return%_3', 'Past_Return%_30', 'Past_Return%_5']\n",
      "Run 3/230 | Train: 2022-02-14 → 2026-01-09 | Test: 2026-01-14 → 2026-01-14 | Train_n=980 | Test_n=1 | (PI Years: 2 - Feats: 6)\n",
      "PI Train: 2022-02-11 → 2026-01-08\n",
      "Ran permutation importance for horizon 2 | Len: 484 | Old: 14 | New: 6\n",
      "['Past_Return%_1', 'Past_Return%_10', 'Past_Return%_2', 'Past_Return%_3', 'Past_Return%_30', 'Past_Return%_5']\n",
      "Run 4/230 | Train: 2022-02-11 → 2026-01-08 | Test: 2026-01-13 → 2026-01-13 | Train_n=980 | Test_n=1 | (PI Years: 2 - Feats: 6)\n",
      "PI Train: 2022-02-10 → 2026-01-07\n",
      "Ran permutation importance for horizon 2 | Len: 484 | Old: 14 | New: 6\n",
      "['Past_Return%_1', 'Past_Return%_10', 'Past_Return%_2', 'Past_Return%_3', 'Past_Return%_30', 'Past_Return%_5']\n",
      "Run 5/230 | Train: 2022-02-10 → 2026-01-07 | Test: 2026-01-12 → 2026-01-12 | Train_n=980 | Test_n=1 | (PI Years: 2 - Feats: 6)\n",
      "PI Train: 2022-02-09 → 2026-01-06\n",
      "Ran permutation importance for horizon 2 | Len: 484 | Old: 14 | New: 6\n",
      "['Past_Return%_1', 'Past_Return%_10', 'Past_Return%_2', 'Past_Return%_3', 'Past_Return%_30', 'Past_Return%_5']\n",
      "Run 6/230 | Train: 2022-02-09 → 2026-01-06 | Test: 2026-01-09 → 2026-01-09 | Train_n=980 | Test_n=1 | (PI Years: 2 - Feats: 6)\n",
      "PI Train: 2022-02-08 → 2026-01-05\n",
      "Ran permutation importance for horizon 2 | Len: 484 | Old: 14 | New: 6\n",
      "['Past_Return%_1', 'Past_Return%_10', 'Past_Return%_2', 'Past_Return%_3', 'Past_Return%_30', 'Past_Return%_5']\n",
      "Run 7/230 | Train: 2022-02-08 → 2026-01-05 | Test: 2026-01-08 → 2026-01-08 | Train_n=980 | Test_n=1 | (PI Years: 2 - Feats: 6)\n",
      "PI Train: 2022-02-07 → 2026-01-02\n",
      "Ran permutation importance for horizon 2 | Len: 484 | Old: 14 | New: 6\n",
      "['Past_Return%_1', 'Past_Return%_10', 'Past_Return%_2', 'Past_Return%_3', 'Past_Return%_30', 'Past_Return%_5']\n",
      "Run 8/230 | Train: 2022-02-07 → 2026-01-02 | Test: 2026-01-07 → 2026-01-07 | Train_n=980 | Test_n=1 | (PI Years: 2 - Feats: 6)\n",
      "PI Train: 2022-02-04 → 2025-12-31\n",
      "Ran permutation importance for horizon 2 | Len: 484 | Old: 14 | New: 6\n",
      "['Past_Return%_1', 'Past_Return%_10', 'Past_Return%_2', 'Past_Return%_3', 'Past_Return%_30', 'Past_Return%_5']\n",
      "Run 9/230 | Train: 2022-02-04 → 2025-12-31 | Test: 2026-01-06 → 2026-01-06 | Train_n=980 | Test_n=1 | (PI Years: 2 - Feats: 6)\n",
      "PI Train: 2022-02-03 → 2025-12-30\n",
      "Ran permutation importance for horizon 2 | Len: 484 | Old: 14 | New: 6\n",
      "['Past_Return%_1', 'Past_Return%_10', 'Past_Return%_2', 'Past_Return%_3', 'Past_Return%_30', 'Past_Return%_5']\n",
      "Run 10/230 | Train: 2022-02-03 → 2025-12-30 | Test: 2026-01-05 → 2026-01-05 | Train_n=980 | Test_n=1 | (PI Years: 2 - Feats: 6)\n",
      "PI Train: 2022-02-02 → 2025-12-29\n",
      "Ran permutation importance for horizon 2 | Len: 484 | Old: 14 | New: 6\n",
      "['Past_Return%_1', 'Past_Return%_10', 'Past_Return%_2', 'Past_Return%_3', 'Past_Return%_30', 'Past_Return%_5']\n",
      "Run 11/230 | Train: 2022-02-02 → 2025-12-29 | Test: 2026-01-02 → 2026-01-02 | Train_n=980 | Test_n=1 | (PI Years: 2 - Feats: 6)\n",
      "PI Train: 2022-02-01 → 2025-12-26\n",
      "Ran permutation importance for horizon 2 | Len: 484 | Old: 14 | New: 6\n",
      "['Past_Return%_1', 'Past_Return%_10', 'Past_Return%_2', 'Past_Return%_3', 'Past_Return%_30', 'Past_Return%_5']\n",
      "Run 12/230 | Train: 2022-02-01 → 2025-12-26 | Test: 2025-12-31 → 2025-12-31 | Train_n=980 | Test_n=1 | (PI Years: 2 - Feats: 6)\n",
      "PI Train: 2022-01-31 → 2025-12-24\n",
      "Ran permutation importance for horizon 2 | Len: 484 | Old: 14 | New: 6\n",
      "['Past_Return%_1', 'Past_Return%_10', 'Past_Return%_2', 'Past_Return%_3', 'Past_Return%_30', 'Past_Return%_5']\n",
      "Run 13/230 | Train: 2022-01-31 → 2025-12-24 | Test: 2025-12-30 → 2025-12-30 | Train_n=980 | Test_n=1 | (PI Years: 2 - Feats: 6)\n",
      "PI Train: 2022-01-28 → 2025-12-23\n",
      "Ran permutation importance for horizon 2 | Len: 484 | Old: 14 | New: 6\n",
      "['Past_Return%_1', 'Past_Return%_10', 'Past_Return%_2', 'Past_Return%_3', 'Past_Return%_30', 'Past_Return%_5']\n",
      "Run 14/230 | Train: 2022-01-28 → 2025-12-23 | Test: 2025-12-29 → 2025-12-29 | Train_n=980 | Test_n=1 | (PI Years: 2 - Feats: 6)\n",
      "PI Train: 2022-01-27 → 2025-12-22\n",
      "Ran permutation importance for horizon 2 | Len: 484 | Old: 14 | New: 6\n",
      "['Past_Return%_1', 'Past_Return%_10', 'Past_Return%_2', 'Past_Return%_3', 'Past_Return%_30', 'Past_Return%_5']\n",
      "Run 15/230 | Train: 2022-01-27 → 2025-12-22 | Test: 2025-12-26 → 2025-12-26 | Train_n=980 | Test_n=1 | (PI Years: 2 - Feats: 6)\n",
      "PI Train: 2022-01-26 → 2025-12-19\n",
      "Ran permutation importance for horizon 2 | Len: 484 | Old: 14 | New: 6\n",
      "['Past_Return%_1', 'Past_Return%_10', 'Past_Return%_2', 'Past_Return%_3', 'Past_Return%_30', 'Past_Return%_5']\n",
      "Run 16/230 | Train: 2022-01-26 → 2025-12-19 | Test: 2025-12-24 → 2025-12-24 | Train_n=980 | Test_n=1 | (PI Years: 2 - Feats: 6)\n",
      "PI Train: 2022-01-25 → 2025-12-18\n",
      "Ran permutation importance for horizon 2 | Len: 484 | Old: 14 | New: 6\n",
      "['Past_Return%_10', 'Past_Return%_2', 'Past_Return%_20', 'Past_Return%_3', 'Past_Return%_30', 'Past_Return%_5']\n",
      "Run 17/230 | Train: 2022-01-25 → 2025-12-18 | Test: 2025-12-23 → 2025-12-23 | Train_n=980 | Test_n=1 | (PI Years: 2 - Feats: 6)\n",
      "PI Train: 2022-01-24 → 2025-12-17\n",
      "Ran permutation importance for horizon 2 | Len: 484 | Old: 14 | New: 6\n",
      "['Past_Return%_10', 'Past_Return%_2', 'Past_Return%_20', 'Past_Return%_3', 'Past_Return%_30', 'Past_Return%_5']\n",
      "Run 18/230 | Train: 2022-01-24 → 2025-12-17 | Test: 2025-12-22 → 2025-12-22 | Train_n=980 | Test_n=1 | (PI Years: 2 - Feats: 6)\n",
      "PI Train: 2022-01-21 → 2025-12-16\n",
      "Ran permutation importance for horizon 2 | Len: 484 | Old: 14 | New: 6\n",
      "['Past_Return%_1', 'Past_Return%_10', 'Past_Return%_2', 'Past_Return%_3', 'Past_Return%_30', 'Past_Return%_5']\n",
      "Run 19/230 | Train: 2022-01-21 → 2025-12-16 | Test: 2025-12-19 → 2025-12-19 | Train_n=980 | Test_n=1 | (PI Years: 2 - Feats: 6)\n",
      "PI Train: 2022-01-20 → 2025-12-15\n",
      "Ran permutation importance for horizon 2 | Len: 484 | Old: 14 | New: 6\n",
      "['Past_Return%_1', 'Past_Return%_10', 'Past_Return%_2', 'Past_Return%_3', 'Past_Return%_30', 'Past_Return%_5']\n",
      "Run 20/230 | Train: 2022-01-20 → 2025-12-15 | Test: 2025-12-18 → 2025-12-18 | Train_n=980 | Test_n=1 | (PI Years: 2 - Feats: 6)\n",
      "PI Train: 2022-01-19 → 2025-12-12\n",
      "Ran permutation importance for horizon 2 | Len: 484 | Old: 14 | New: 6\n",
      "['Past_Return%_1', 'Past_Return%_10', 'Past_Return%_2', 'Past_Return%_3', 'Past_Return%_30', 'Past_Return%_5']\n",
      "Run 21/230 | Train: 2022-01-19 → 2025-12-12 | Test: 2025-12-17 → 2025-12-17 | Train_n=980 | Test_n=1 | (PI Years: 2 - Feats: 6)\n",
      "PI Train: 2022-01-18 → 2025-12-11\n",
      "Ran permutation importance for horizon 2 | Len: 484 | Old: 14 | New: 6\n",
      "['Past_Return%_1', 'Past_Return%_10', 'Past_Return%_2', 'Past_Return%_3', 'Past_Return%_30', 'Past_Return%_5']\n",
      "Run 22/230 | Train: 2022-01-18 → 2025-12-11 | Test: 2025-12-16 → 2025-12-16 | Train_n=980 | Test_n=1 | (PI Years: 2 - Feats: 6)\n",
      "PI Train: 2022-01-14 → 2025-12-10\n",
      "Ran permutation importance for horizon 2 | Len: 484 | Old: 14 | New: 6\n",
      "['Past_Return%_1', 'Past_Return%_10', 'Past_Return%_2', 'Past_Return%_3', 'Past_Return%_30', 'Past_Return%_5']\n",
      "Run 23/230 | Train: 2022-01-14 → 2025-12-10 | Test: 2025-12-15 → 2025-12-15 | Train_n=980 | Test_n=1 | (PI Years: 2 - Feats: 6)\n",
      "PI Train: 2022-01-13 → 2025-12-09\n",
      "Ran permutation importance for horizon 2 | Len: 484 | Old: 14 | New: 6\n",
      "['Past_Return%_1', 'Past_Return%_10', 'Past_Return%_2', 'Past_Return%_3', 'Past_Return%_30', 'Past_Return%_5']\n",
      "Run 24/230 | Train: 2022-01-13 → 2025-12-09 | Test: 2025-12-12 → 2025-12-12 | Train_n=980 | Test_n=1 | (PI Years: 2 - Feats: 6)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[419], line 328\u001b[0m\n\u001b[1;32m    324\u001b[0m df_final \u001b[38;5;241m=\u001b[39m df_main\u001b[38;5;241m.\u001b[39miloc[r:]\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[1;32m    326\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m train_year \u001b[38;5;129;01min\u001b[39;00m train_years:\n\u001b[0;32m--> 328\u001b[0m     df_scores \u001b[38;5;241m=\u001b[39m \u001b[43mwalkback_runs\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    329\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdf_final\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    330\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfeature_cols\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbase_cols\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    331\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtarget_col\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtarget_col\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    332\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdate_col\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mDate\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    333\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrain_years\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_year\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    334\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtest_days\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest_day\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    335\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstep_days\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest_day\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    336\u001b[0m \u001b[43m        \u001b[49m\u001b[43mruns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mruns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    337\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhorizon_days\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    338\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpurge_days\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    339\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfill_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    340\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    342\u001b[0m     df_scores[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfeature_set\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpast_returns\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;66;03m#list_name\u001b[39;00m\n\u001b[1;32m    343\u001b[0m     df_scores[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhorizon\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m r\n",
      "Cell \u001b[0;32mIn[419], line 161\u001b[0m, in \u001b[0;36mwalkback_runs\u001b[0;34m(df, feature_cols, target_col, date_col, train_years, test_days, step_days, runs, horizon_days, purge_days, fill_inf)\u001b[0m\n\u001b[1;32m    158\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m model_name, model \u001b[38;5;129;01min\u001b[39;00m models\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m    159\u001b[0m     \u001b[38;5;66;03m#start_time = time.time()\u001b[39;00m\n\u001b[1;32m    160\u001b[0m     m \u001b[38;5;241m=\u001b[39m clone(model)\n\u001b[0;32m--> 161\u001b[0m     \u001b[43mm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    163\u001b[0m     preds \u001b[38;5;241m=\u001b[39m m\u001b[38;5;241m.\u001b[39mpredict(X_test)\n\u001b[1;32m    164\u001b[0m     proba \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mnan\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/xgboost/core.py:726\u001b[0m, in \u001b[0;36mrequire_keyword_args.<locals>.throw_if.<locals>.inner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    724\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k, arg \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(sig\u001b[38;5;241m.\u001b[39mparameters, args):\n\u001b[1;32m    725\u001b[0m     kwargs[k] \u001b[38;5;241m=\u001b[39m arg\n\u001b[0;32m--> 726\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/xgboost/sklearn.py:1599\u001b[0m, in \u001b[0;36mXGBClassifier.fit\u001b[0;34m(self, X, y, sample_weight, base_margin, eval_set, verbose, xgb_model, sample_weight_eval_set, base_margin_eval_set, feature_weights)\u001b[0m\n\u001b[1;32m   1579\u001b[0m model, metric, params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_configure_fit(xgb_model, params)\n\u001b[1;32m   1580\u001b[0m train_dmatrix, evals \u001b[38;5;241m=\u001b[39m _wrap_evaluation_matrices(\n\u001b[1;32m   1581\u001b[0m     missing\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmissing,\n\u001b[1;32m   1582\u001b[0m     X\u001b[38;5;241m=\u001b[39mX,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1596\u001b[0m     feature_types\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeature_types,\n\u001b[1;32m   1597\u001b[0m )\n\u001b[0;32m-> 1599\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_Booster \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1600\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1601\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_dmatrix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1602\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_num_boosting_rounds\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1603\u001b[0m \u001b[43m    \u001b[49m\u001b[43mevals\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mevals\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1604\u001b[0m \u001b[43m    \u001b[49m\u001b[43mearly_stopping_rounds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mearly_stopping_rounds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1605\u001b[0m \u001b[43m    \u001b[49m\u001b[43mevals_result\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mevals_result\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1606\u001b[0m \u001b[43m    \u001b[49m\u001b[43mobj\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1607\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcustom_metric\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetric\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1608\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1609\u001b[0m \u001b[43m    \u001b[49m\u001b[43mxgb_model\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1610\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1611\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1613\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobjective):\n\u001b[1;32m   1614\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobjective \u001b[38;5;241m=\u001b[39m params[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mobjective\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/xgboost/core.py:726\u001b[0m, in \u001b[0;36mrequire_keyword_args.<locals>.throw_if.<locals>.inner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    724\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k, arg \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(sig\u001b[38;5;241m.\u001b[39mparameters, args):\n\u001b[1;32m    725\u001b[0m     kwargs[k] \u001b[38;5;241m=\u001b[39m arg\n\u001b[0;32m--> 726\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/xgboost/training.py:181\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(params, dtrain, num_boost_round, evals, obj, feval, maximize, early_stopping_rounds, evals_result, verbose_eval, xgb_model, callbacks, custom_metric)\u001b[0m\n\u001b[1;32m    179\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cb_container\u001b[38;5;241m.\u001b[39mbefore_iteration(bst, i, dtrain, evals):\n\u001b[1;32m    180\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m--> 181\u001b[0m \u001b[43mbst\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43miteration\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfobj\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cb_container\u001b[38;5;241m.\u001b[39mafter_iteration(bst, i, dtrain, evals):\n\u001b[1;32m    183\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/xgboost/core.py:2101\u001b[0m, in \u001b[0;36mBooster.update\u001b[0;34m(self, dtrain, iteration, fobj)\u001b[0m\n\u001b[1;32m   2097\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_assign_dmatrix_features(dtrain)\n\u001b[1;32m   2099\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m fobj \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   2100\u001b[0m     _check_call(\n\u001b[0;32m-> 2101\u001b[0m         \u001b[43m_LIB\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mXGBoosterUpdateOneIter\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2102\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mctypes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mc_int\u001b[49m\u001b[43m(\u001b[49m\u001b[43miteration\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtrain\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle\u001b[49m\n\u001b[1;32m   2103\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2104\u001b[0m     )\n\u001b[1;32m   2105\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   2106\u001b[0m     pred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredict(dtrain, output_margin\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, training\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# -----------------------------\n",
    "# Feature Sets\n",
    "# -----------------------------\n",
    "ma_all_cols = feature_sets['ma']\n",
    "ma_lag = [c for c in ma_all_cols if \"lag\" in c.lower()]\n",
    "ma_rel = [c for c in ma_all_cols if \"rel_\" in c.lower()]\n",
    "ma_sma = [c for c in ma_all_cols if (\"sma_\" in c.lower()) and (\"lag\" not in c.lower())]\n",
    "ma_num = [c for c in ma_all_cols if (\"num\" in c.lower()) or (\"since\" in c.lower())]\n",
    "rsi_cols = feature_sets['rsi']\n",
    "macd_cols = feature_sets['macd']\n",
    "volu_cols = feature_sets['volume']\n",
    "atr_adx_cols = feature_sets['atr_adx']\n",
    "vola_cols = feature_sets['volatility']\n",
    "vix_skew_cols = feature_sets['vix_skew']\n",
    "experimental_slope_cols = feature_sets['experimental_slope']\n",
    "\n",
    "sets = [ma_lag, ma_rel, ma_sma, ma_num, rsi_cols + macd_cols, volu_cols, atr_adx_cols + vola_cols, vix_skew_cols, experimental_slope_cols]\n",
    "set_names = [\"ma_lag\", \"ma_rel\", \"ma_sma\", \"ma_num\", \"rsi_macd\", \"volu\", \"atr_adx\" + \"vola\", \"vix_skew\", \"experimental_slope\"]\n",
    "\n",
    "# -----------------------------\n",
    "# Models\n",
    "# -----------------------------\n",
    "models = {\n",
    "    #\"xgboost-4\": XGBClassifier(n_estimators=400, random_state=42, n_jobs=-1),\n",
    "    #\"xgboost-6\": XGBClassifier(n_estimators=600, random_state=42, n_jobs=-1),\n",
    "    \"xgb_first_pass\": XGBClassifier(\n",
    "    n_estimators=800,\n",
    "    learning_rate=0.02,\n",
    "    max_depth=5,\n",
    "    min_child_weight=10,\n",
    "    subsample=0.7,\n",
    "    colsample_bytree=0.7,\n",
    "    gamma=0.3,\n",
    "    reg_alpha=0.5,\n",
    "    reg_lambda=15,\n",
    "    tree_method=\"hist\",\n",
    "    random_state=42,\n",
    "    n_jobs=-1)\n",
    "}\n",
    "\n",
    "# -----------------------------\n",
    "# Run grid (feature sets x horizon x train_years, etc.)\n",
    "# -----------------------------\n",
    "returns = [2]#, 10, 30]#[2, 5, 10, 20, 30]\n",
    "#train_years = [5]#[3, 5, 7] \n",
    "days_assessed = 230\n",
    "test_days = [1]\n",
    "results= []\n",
    "results_df = pd.DataFrame()\n",
    "models = {\"xgboost-2\": XGBClassifier(n_estimators=200, random_state=42, n_jobs=-1)}\n",
    "\n",
    "def _compute_dist(y):\n",
    "    \"\"\"Distribution stats for y in {0,1}.\"\"\"\n",
    "    n = int(len(y))\n",
    "    n_pos = int((y == 1).sum())\n",
    "    n_neg = int((y == 0).sum())\n",
    "    return {\n",
    "        \"test_n\": n,\n",
    "        \"test_pos_n\": n_pos,\n",
    "        \"test_neg_n\": n_neg,\n",
    "        \"test_pos_frac\": (n_pos / n) if n else np.nan,\n",
    "        \"test_neg_frac\": (n_neg / n) if n else np.nan,\n",
    "    }\n",
    "\n",
    "def walkback_runs(\n",
    "    df,\n",
    "    feature_cols,\n",
    "    target_col,\n",
    "    *,\n",
    "    date_col=\"Date\",\n",
    "    train_years=6,\n",
    "    test_days=5,\n",
    "    step_days=5,\n",
    "    runs=20,\n",
    "    horizon_days=1,        # r (used for purge)\n",
    "    purge_days=None,       # defaults to horizon_days\n",
    "    fill_inf=0.0,\n",
    "):\n",
    "    \"\"\"\n",
    "    Deployment-aligned evaluation:\n",
    "      - For each run, take a 5-day OOT test window stepping back by 5 days.\n",
    "      - Train on the prior N years (fixed-length window) ending right before test.\n",
    "      - Purge 'purge_days' from the end of train to avoid overlap leakage for forward-return labels.\n",
    "      - Score ONLY on the OOT test window (distribution + metrics).\n",
    "    Returns: long DataFrame with one row per (feature_set/run/model).\n",
    "    \"\"\"\n",
    "    rows = []\n",
    "\n",
    "    for k in range(runs):\n",
    "\n",
    "        dfw = df.sort_values(\"Date\").reset_index(drop=True).copy()\n",
    "\n",
    "        n = len(dfw)\n",
    "        train_size = 245 * int(train_years)\n",
    "        test_size = int(test_days)\n",
    "        step = int(step_days)\n",
    "        purge = int(purge_days) if purge_days is not None else 0 #int(horizon_days)\n",
    "        test_end = n - k * step\n",
    "        test_start = test_end - test_size\n",
    "\n",
    "        if test_start < 0:\n",
    "            break\n",
    "\n",
    "        train_end = test_start - purge\n",
    "        train_start = train_end - train_size\n",
    "        if train_start < 0 or train_end <= train_start:\n",
    "            break\n",
    "        \n",
    "        dates = dfw[date_col].to_numpy() if date_col in dfw.columns else None\n",
    "        dfpi = dfw[train_start:train_end].copy()\n",
    "\n",
    "        print(f\"PI Train: {dates[train_start]} → {dates[train_end-1]}\")\n",
    "\n",
    "        for pi_year in pi_years:\n",
    "\n",
    "            for min_feat in min_feats:\n",
    "                \n",
    "                #feature_cols = [c for c in feature_cols if c not in new_cols]\n",
    "\n",
    "                perm_cols = perm_list(\n",
    "                    df=dfpi,\n",
    "                    feature_cols=feature_cols,\n",
    "                    target_col=target_col,\n",
    "                    date_col=\"Date\",\n",
    "                    purge_days=r, \n",
    "                    fill_inf=0.0,\n",
    "                    pi_year=pi_year,\n",
    "                    min_feats=min_feat\n",
    "                )\n",
    "                print(sorted(perm_cols))\n",
    "                #perm_cols += new_cols\n",
    "                \n",
    "                # Drop any accidental return cols from features (belt+suspenders)\n",
    "                safe_feature_cols = [c for c in perm_cols if not c.startswith(\"Return\")]\n",
    "\n",
    "                # Basic numeric cleaning\n",
    "                dfw[safe_feature_cols] = dfw[safe_feature_cols].replace([np.inf, -np.inf], fill_inf)\n",
    "\n",
    "                X_all = dfw[safe_feature_cols].to_numpy()\n",
    "                #y_all = _to_binary(dfw[target_col].to_numpy())\n",
    "                y_all = dfw[target_col].to_numpy()\n",
    "\n",
    "                print(\n",
    "                    f\"Run {k+1}/{runs} | \"\n",
    "                    f\"Train: {dates[train_start]} → {dates[train_end-1]} | \"\n",
    "                    f\"Test: {dates[test_start]} → {dates[test_end-1]} | \"\n",
    "                    f\"Train_n={train_end-train_start} | Test_n={test_end-test_start} | \"\n",
    "                    f\"(PI Years: {pi_year} - Feats: {min_feat})\"\n",
    "                )\n",
    "\n",
    "                X_train = X_all[train_start:train_end]\n",
    "                y_train = y_all[train_start:train_end]\n",
    "                X_test  = X_all[test_start:test_end]\n",
    "                y_test  = y_all[test_start:test_end]\n",
    "\n",
    "                dist = _compute_dist(y_test)\n",
    "\n",
    "                for model_name, model in models.items():\n",
    "                    #start_time = time.time()\n",
    "                    m = clone(model)\n",
    "                    m.fit(X_train, y_train)\n",
    "\n",
    "                    preds = m.predict(X_test)\n",
    "                    proba = np.nan\n",
    "                    if hasattr(m, \"predict_proba\"):\n",
    "                        proba = float(m.predict_proba(X_test)[0, 1])   # prob(class=1)\n",
    "                    elif hasattr(m, \"decision_function\"):\n",
    "                        s = float(m.decision_function(X_test)[0])\n",
    "                        proba = float(1.0 / (1.0 + np.exp(-s)))        # squash to (0,1)\n",
    "                    proba = np.nan if np.isnan(proba) else round(round(proba / 0.05) * 0.05, 2)\n",
    "\n",
    "                    rows.append({\n",
    "                        \"run\": k + 1,\n",
    "                        \"model\": model_name,\n",
    "                        \"test_days\": test_days,\n",
    "                        \"pred\": round(proba,2),\n",
    "                        \"acc\": float(accuracy_score(y_test, preds)),\n",
    "                        **dist,\n",
    "                        \"train_n\": int(len(y_train)),\n",
    "                        \"train_start\": dates[train_start] if dates is not None else train_start,\n",
    "                        \"train_end\": dates[train_end - 1] if dates is not None else train_end - 1,\n",
    "                        \"test_start\": dates[test_start] if dates is not None else test_start,\n",
    "                        \"test_end\": dates[test_end - 1] if dates is not None else test_end - 1,\n",
    "                        \"train_years\": train_years,\n",
    "                        \"n_features\": len(safe_feature_cols),\n",
    "                        \"pi_size\": pi_year,\n",
    "                        \"min_feats\": min_feat\n",
    "                    })\n",
    "        \n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "def perm_list(\n",
    "    df,\n",
    "    feature_cols,\n",
    "    target_col,\n",
    "    *,\n",
    "    date_col=\"Date\",\n",
    "    purge_days=None, \n",
    "    fill_inf=0.0,\n",
    "    pi_year=1,\n",
    "    min_feats=6,\n",
    "    k=1\n",
    "):\n",
    "\n",
    "    dfw = df.sort_values(\"Date\").reset_index(drop=True).copy()\n",
    "\n",
    "    # Drop any accidental return cols from features (belt+suspenders)\n",
    "    safe_feature_cols = [c for c in feature_cols if not (c.startswith(\"Return\"))]\n",
    "\n",
    "    # Basic numeric cleaning\n",
    "    dfw[safe_feature_cols] = dfw[safe_feature_cols].replace([np.inf, -np.inf], fill_inf)\n",
    "\n",
    "    \"\"\"\n",
    "    test_size = int(test_days)\n",
    "    step = int(step_days)\n",
    "    purge = int(purge_days) if purge_days is not None else 0 #int(horizon_days)\n",
    "    \"\"\"\n",
    "\n",
    "    X_train = dfw[safe_feature_cols].to_numpy()\n",
    "    #y_all = _to_binary(dfw[target_col].to_numpy())\n",
    "    y_train = dfw[target_col].to_numpy()\n",
    "    dates = dfw[date_col].to_numpy() if date_col in dfw.columns else None\n",
    "\n",
    "    \"\"\"\n",
    "    test_end = n - k * step\n",
    "    test_start = test_end - test_size\n",
    "    train_end = test_start - purge\n",
    "    train_start = train_end - train_size\n",
    "    \n",
    "\n",
    "    X_train = X_all[train_start:train_end]\n",
    "    y_train = y_all[train_start:train_end]\n",
    "    \"\"\"\n",
    "    \n",
    "    #N_PI = int(len(X_train) * perc_train)\n",
    "    N_PI = int(242 * pi_year)\n",
    "    dates_pi = dates[-N_PI:]\n",
    "    X_pi = X_train[-N_PI:]\n",
    "    y_pi = y_train[-N_PI:]\n",
    "\n",
    "    #print(f\"{max(dates_pi)} - {min(dates_pi)}\")\n",
    "    # fit model\n",
    "    m = clone(model).fit(X_train, y_train)\n",
    "\n",
    "    # permutation importance on training-only slice\n",
    "    pi = permutation_importance(\n",
    "        m,\n",
    "        X_pi,\n",
    "        y_pi,\n",
    "        scoring=\"neg_log_loss\",   # or \"accuracy\", \"neg_log_loss\", etc.\n",
    "        n_repeats=10,\n",
    "        random_state=42,\n",
    "        n_jobs=-1,\n",
    "    )\n",
    "\n",
    "    # pi.importances_mean aligns to feature_cols order\n",
    "    pi_df = pd.DataFrame({\n",
    "        \"feature\": feature_cols,                 # same order used to build X_train\n",
    "        \"pi_mean\": pi.importances_mean,\n",
    "        \"pi_std\":  pi.importances_std,\n",
    "    }).sort_values(\"pi_mean\", ascending=False)\n",
    "\n",
    "    # keep only features with PI > 0\n",
    "    pi_cols = pi_df['feature'][pi_df['pi_mean'] > .01].to_list()\n",
    "\n",
    "    if len(pi_cols) < min_feats:\n",
    "        pi_cols = (\n",
    "            pi_df.sort_values(\"pi_mean\", ascending=False)\n",
    "                .head(min_feats)[\"feature\"]\n",
    "                .tolist()\n",
    "        )\n",
    "    print(f\"Ran permutation importance for horizon {purge_days} | Len: {N_PI} | Old: {len(feature_cols)} | New: {len(pi_cols)}\")\n",
    "    \n",
    "    return pi_cols\n",
    "\n",
    "for test_day in test_days:\n",
    "\n",
    "    runs = int(days_assessed / test_day)\n",
    "\n",
    "    for r in returns:\n",
    "\n",
    "        if r == 2:\n",
    "            base_cols = experimental_slope_cols + ma_lag + rsi_cols + macd_cols + volu_cols\n",
    "            base_cols = ma_lag + ma_rel + ma_sma + ma_num + rsi_cols + macd_cols + volu_cols + atr_adx_cols + vola_cols + vix_skew_cols + experimental_slope_cols + new_cols\n",
    "            base_cols = new_cols\n",
    "            ### Best Model Config ###\n",
    "            train_years = [4]\n",
    "            pi_years = [2]\n",
    "            min_feats = [6]\n",
    "            models = {\"xgboost-2\": XGBClassifier(n_estimators=200, random_state=42, n_jobs=-1)}\n",
    "\n",
    "        elif r == 5:\n",
    "            base_cols = experimental_slope_cols + ma_lag + ma_num + rsi_cols + macd_cols + volu_cols\n",
    "            base_cols = ma_lag + ma_rel + ma_sma + ma_num + rsi_cols + macd_cols + volu_cols + atr_adx_cols + vola_cols + vix_skew_cols + experimental_slope_cols\n",
    "            list_name = \"initial+atradx\" #worse\n",
    "            list_name = \"initial+sma\" #worse\n",
    "            list_name = \"initial+vixskew\" #much worse\n",
    "            #train_years = 5\n",
    "            #cols = atr_adx_cols + vola_cols + experimental_slope_cols + ma_lag + ma_num + ma_rel + ma_sma + rsi_cols + macd_cols + volu_cols + vix_skew_cols\n",
    "        elif r == 10:\n",
    "            base_cols = atr_adx_cols + vola_cols + ma_num + volu_cols + ma_sma\n",
    "            base_cols = ma_lag + ma_rel + ma_sma + ma_num + rsi_cols + macd_cols + volu_cols + atr_adx_cols + vola_cols + vix_skew_cols + experimental_slope_cols\n",
    "            list_name = \"initial+sma\" # better\n",
    "            list_name = \"initial+sma+lag\" #worse\n",
    "            #train_years = 5\n",
    "            #cols = atr_adx_cols + vola_cols + ma_num + ma_rel + ma_sma + volu_cols + vix_skew_cols\n",
    "        elif r == 20:\n",
    "            base_cols = atr_adx_cols + vola_cols + ma_num + ma_sma\n",
    "            #base_cols = ma_lag + ma_rel + ma_sma + ma_num + rsi_cols + macd_cols + volu_cols + atr_adx_cols + vola_cols + vix_skew_cols + experimental_slope_cols\n",
    "            list_name = \"initial+volu\" #worse\n",
    "            list_name = \"initial+lag\" #much worse\n",
    "            #train_years = 6\n",
    "        else:\n",
    "            base_cols = atr_adx_cols + vola_cols + ma_num + ma_sma + volu_cols + rsi_cols + macd_cols\n",
    "            base_cols = ma_lag + ma_rel + ma_sma + ma_num + rsi_cols + macd_cols + volu_cols + atr_adx_cols + vola_cols + vix_skew_cols + experimental_slope_cols\n",
    "            list_name = \"initial+volu\" #worse\n",
    "            list_name = \"initial-rsimacd+volu\" #better\n",
    "            #train_years = 5\n",
    "            models = {\"xgboost-2\": XGBClassifier(n_estimators=200, random_state=42, n_jobs=-1)}\n",
    "\n",
    "        target_col = f\"Return_{r}\"\n",
    "        # Trime unknown (recent) outcomes\n",
    "        df_final = df_main.iloc[r:].copy()\n",
    "\n",
    "        for train_year in train_years:\n",
    "\n",
    "            df_scores = walkback_runs(\n",
    "                df=df_final,\n",
    "                feature_cols=base_cols,\n",
    "                target_col=target_col,\n",
    "                date_col=\"Date\",\n",
    "                train_years=train_year,\n",
    "                test_days=test_day,\n",
    "                step_days=test_day,\n",
    "                runs=runs,\n",
    "                horizon_days=r,\n",
    "                purge_days=r, \n",
    "                fill_inf=0.0,\n",
    "            )\n",
    "\n",
    "            df_scores[\"feature_set\"] = \"past_returns\" #list_name\n",
    "            df_scores[\"horizon\"] = r\n",
    "\n",
    "            results.append(df_scores)\n",
    "\n",
    "results_df = pd.concat(results, ignore_index=True)\n",
    "#results_df.to_csv(\"baseline.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 413,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"horizon2_best.csv\")\n",
    "cols = ['model', 'test_days', 'pred', 'acc', 'test_n', 'test_pos_n', 'train_n', 'test_start', 'test_end', 'train_years', 'feature_set', 'horizon',\n",
    "        'pi_size', 'min_feats']\n",
    "df_new = results_df[cols].copy()\n",
    "df_concat = pd.concat([df[cols], df_new], ignore_index=True)\n",
    "df_concat.to_csv('horizon2_new.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['run', 'model', 'test_days', 'pred', 'acc', 'test_n', 'test_pos_n',\n",
       "       'test_neg_n', 'test_pos_frac', 'test_neg_frac', 'train_n',\n",
       "       'train_start', 'train_end', 'test_start', 'test_end', 'train_years',\n",
       "       'n_features', 'pi_size', 'min_feats', 'feature_set', 'horizon'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 356,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"best_models_perf.csv\")\n",
    "#df.to_csv('best_models_backup.csv', index=False)\n",
    "cols = ['model', 'test_days', 'pred', 'acc', 'test_n', 'test_pos_n', 'train_n', 'test_start', 'test_end', 'train_years', 'feature_set', 'horizon']\n",
    "df_new = results_df[cols].copy()\n",
    "df_concat = pd.concat([df, df_new], ignore_index=True)\n",
    "df_concat.to_csv('best_models_perf.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_concat.drop_duplicates()\n",
    "df_concat.drop_duplicates().to_csv('best_models_perf.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['model', 'test_days', 'pred', 'acc', 'test_n', 'test_pos_n', 'train_n', 'test_start', 'test_end', 'train_years', 'feature_set', 'horizon']\n",
    "df_new = results_df[cols].copy()\n",
    "df_new.to_csv('performance.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
