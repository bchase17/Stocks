{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9640b56",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "import train_flow\n",
    "import importlib\n",
    "importlib.reload(train_flow)\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4d89da1",
   "metadata": {},
   "source": [
    "# Feature Engineering for net new features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 406,
   "id": "b83ee60c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available Feature Sets: dict_keys(['ma', 'rsi', 'macd', 'volume', 'atr_adx', 'volatility', 'vix_skew', 'experimental_slope', 'past_return'])\n"
     ]
    }
   ],
   "source": [
    "# --------- #\n",
    "# LEAVE ME  #\n",
    "# --------- #\n",
    "ticker = 'QQQ'\n",
    "include_minute_feats = \"N\"\n",
    "returns = [1, 2, 3, 5, 10, 20, 30]\n",
    "df_daily, feature_sets, return_cols, daily_cols, feature_dict, features = train_flow.import_data(ticker, include_minute_feats, returns)\n",
    "\n",
    "# ---------- #\n",
    "# UPDATE ME  #\n",
    "# ---------- #\n",
    "# Add any new features\n",
    "\"\"\"\n",
    "df_daily[[f\"{c}_sum10\" for c in df_daily.columns if c.startswith(\"Past_Return_\")]] = (df_daily.sort_values(by=\"Date\", ascending=True).filter(like=\"Past_Return_\").rolling(10, min_periods=1).sum())\n",
    "past_ret_cols = [c for c in df_daily.columns if c.startswith(\"Past_Return%\") or c.endswith(\"sum10\")]\n",
    "past_perc_cols = [c for c in df_daily.columns if c.startswith(\"Past_Return%\")]\n",
    "past_sum_cols = [c for c in df_daily.columns if c.endswith(\"sum10\")]\n",
    "#cutoff_date = '2026-02-06' # Unlessretraining all baseline models, leave alone\n",
    "\"\"\"\n",
    "# --------- #\n",
    "# LEAVE ME  #\n",
    "# --------- #\n",
    "df_main = df_daily.copy()#[df_daily['Date'] <= cutoff_date].copy() #cutoff date for baseline performance metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f8a8226",
   "metadata": {},
   "source": [
    "# Run Train_Flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cbdc199",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ma_lag-volu\n",
      "Running for horizon 5 | run_separately\n",
      "Run 1/50 | Train: 2023-02-22 → 2026-01-27 | Test: 2026-02-04 → 2026-02-10 | Train_n=735 | Test_n=5 | (PI Years: 1.5 - Feats: 5)\n",
      "Run 2/50 | Train: 2023-02-14 → 2026-01-20 | Test: 2026-01-28 → 2026-02-03 | Train_n=735 | Test_n=5 | (PI Years: 1.5 - Feats: 5)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[416], line 41\u001b[0m\n\u001b[1;32m     38\u001b[0m models \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxgboost-tr3-d6-c1-lr:.1\u001b[39m\u001b[38;5;124m\"\u001b[39m: XGBClassifier(n_estimators\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m300\u001b[39m, max_depth\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m6\u001b[39m, colsample_bytree\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, min_child_weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m, \n\u001b[1;32m     39\u001b[0m                                                     learning_rate\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m.1\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m, n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)}\n\u001b[1;32m     40\u001b[0m models \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxgboost-3\u001b[39m\u001b[38;5;124m\"\u001b[39m: XGBClassifier(n_estimators\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m300\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m, n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)}\n\u001b[0;32m---> 41\u001b[0m results_df \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_flow\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_train_flow\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_day\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdays_assessed\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_horizons\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpi_handlings\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeature_cols\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdf_main\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     42\u001b[0m \u001b[43m                \u001b[49m\u001b[43mtrain_years\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpi_years\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmin_feats\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlist_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeature_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     44\u001b[0m master_results\u001b[38;5;241m.\u001b[39mappend(results_df)\n\u001b[1;32m     45\u001b[0m     \u001b[38;5;66;03m#master_perm.append(perm_df)\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/Stocks/train_flow.py:391\u001b[0m, in \u001b[0;36mrun_train_flow\u001b[0;34m(test_day, days_assessed, returns, pi_handlings, feature_cols, df, models, train_years, pi_years, min_feats, list_name, feature_dict, groups)\u001b[0m\n\u001b[1;32m    387\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m train_year \u001b[38;5;129;01min\u001b[39;00m train_years:\n\u001b[1;32m    389\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRunning for horizon \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mr\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m | \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpi_handling\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 391\u001b[0m     df_scores \u001b[38;5;241m=\u001b[39m \u001b[43mwalkback_runs\u001b[49m\u001b[43m(\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m#, perm_df\u001b[39;49;00m\n\u001b[1;32m    392\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdf_final\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    393\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    394\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfeature_cols\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfeature_cols\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    395\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtarget_col\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtarget_col\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    396\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpi_years\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpi_years\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    397\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdate_col\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mDate\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    398\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrain_years\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_year\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    399\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtest_days\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest_day\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    400\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstep_days\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest_day\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    401\u001b[0m \u001b[43m        \u001b[49m\u001b[43mruns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mruns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    402\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpurge_days\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    403\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfill_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    404\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmin_feats\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmin_feats\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    405\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpi_handling\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpi_handling\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    406\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfeature_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfeature_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    407\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgroups\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    408\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    410\u001b[0m     df_scores[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfeature_set\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m list_name\n\u001b[1;32m    411\u001b[0m     df_scores[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhorizon\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m r\n",
      "File \u001b[0;32m~/Documents/Stocks/train_flow.py:100\u001b[0m, in \u001b[0;36mwalkback_runs\u001b[0;34m(df, models, feature_cols, target_col, pi_years, date_col, train_years, test_days, step_days, runs, purge_days, fill_inf, min_feats, pi_handling, feature_dict, groups)\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m g \u001b[38;5;129;01min\u001b[39;00m groups:\n\u001b[1;32m     98\u001b[0m     group_cols \u001b[38;5;241m=\u001b[39m feature_dict[g]\n\u001b[0;32m--> 100\u001b[0m     perm_cols, p_df \u001b[38;5;241m=\u001b[39m \u001b[43mperm_list\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    101\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdfpi\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    102\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfeature_cols\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup_cols\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    103\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtarget_col\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtarget_col\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    104\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    105\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfill_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    106\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpi_year\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpi_year\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    107\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmin_feats\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmin_feat\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    108\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfeat_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mg\u001b[49m\n\u001b[1;32m    109\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    111\u001b[0m     final_features \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m perm_cols\n\u001b[1;32m    112\u001b[0m     all_perm_dfs\u001b[38;5;241m.\u001b[39mappend(p_df)\n",
      "File \u001b[0;32m~/Documents/Stocks/train_flow.py:313\u001b[0m, in \u001b[0;36mperm_list\u001b[0;34m(df, feature_cols, target_col, model, fill_inf, pi_year, min_feats, feat_type)\u001b[0m\n\u001b[1;32m    310\u001b[0m m \u001b[38;5;241m=\u001b[39m clone(model)\u001b[38;5;241m.\u001b[39mfit(X_train, y_train)\n\u001b[1;32m    312\u001b[0m \u001b[38;5;66;03m# permutation importance on training-only slice\u001b[39;00m\n\u001b[0;32m--> 313\u001b[0m pi \u001b[38;5;241m=\u001b[39m \u001b[43mpermutation_importance\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    314\u001b[0m \u001b[43m    \u001b[49m\u001b[43mm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    315\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX_pi\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    316\u001b[0m \u001b[43m    \u001b[49m\u001b[43my_pi\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    317\u001b[0m \u001b[43m    \u001b[49m\u001b[43mscoring\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mbalanced_accuracy\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m   \u001b[49m\u001b[38;5;66;43;03m# or \"accuracy\", \"neg_log_loss\", etc.\u001b[39;49;00m\n\u001b[1;32m    318\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_repeats\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    319\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m42\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    320\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    321\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    323\u001b[0m \u001b[38;5;66;03m# pi.importances_mean aligns to feature_cols order\u001b[39;00m\n\u001b[1;32m    324\u001b[0m pi_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame({\n\u001b[1;32m    325\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfeature\u001b[39m\u001b[38;5;124m\"\u001b[39m: feature_cols,                 \u001b[38;5;66;03m# same order used to build X_train\u001b[39;00m\n\u001b[1;32m    326\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpi_mean\u001b[39m\u001b[38;5;124m\"\u001b[39m: pi\u001b[38;5;241m.\u001b[39mimportances_mean,\n\u001b[1;32m    327\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpi_std\u001b[39m\u001b[38;5;124m\"\u001b[39m:  pi\u001b[38;5;241m.\u001b[39mimportances_std,\n\u001b[1;32m    328\u001b[0m })\u001b[38;5;241m.\u001b[39msort_values(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpi_mean\u001b[39m\u001b[38;5;124m\"\u001b[39m, ascending\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/sklearn/utils/_param_validation.py:216\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    210\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    211\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m    212\u001b[0m         skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m    213\u001b[0m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m    214\u001b[0m         )\n\u001b[1;32m    215\u001b[0m     ):\n\u001b[0;32m--> 216\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    217\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[1;32m    219\u001b[0m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[1;32m    220\u001b[0m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[1;32m    221\u001b[0m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[1;32m    222\u001b[0m     msg \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\n\u001b[1;32m    223\u001b[0m         \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw+ must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    224\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    225\u001b[0m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[1;32m    226\u001b[0m     )\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/sklearn/inspection/_permutation_importance.py:287\u001b[0m, in \u001b[0;36mpermutation_importance\u001b[0;34m(estimator, X, y, scoring, n_repeats, n_jobs, random_state, sample_weight, max_samples)\u001b[0m\n\u001b[1;32m    284\u001b[0m scorer \u001b[38;5;241m=\u001b[39m check_scoring(estimator, scoring\u001b[38;5;241m=\u001b[39mscoring)\n\u001b[1;32m    285\u001b[0m baseline_score \u001b[38;5;241m=\u001b[39m _weights_scorer(scorer, estimator, X, y, sample_weight)\n\u001b[0;32m--> 287\u001b[0m scores \u001b[38;5;241m=\u001b[39m \u001b[43mParallel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    288\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_calculate_permutation_scores\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    289\u001b[0m \u001b[43m        \u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    290\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    291\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    292\u001b[0m \u001b[43m        \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    293\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcol_idx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    294\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrandom_seed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    295\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_repeats\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    296\u001b[0m \u001b[43m        \u001b[49m\u001b[43mscorer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    297\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_samples\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    298\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    299\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mcol_idx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mrange\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    300\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    302\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(baseline_score, \u001b[38;5;28mdict\u001b[39m):\n\u001b[1;32m    303\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[1;32m    304\u001b[0m         name: _create_importances_bunch(\n\u001b[1;32m    305\u001b[0m             baseline_score[name],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    309\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m baseline_score\n\u001b[1;32m    310\u001b[0m     }\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/sklearn/utils/parallel.py:77\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m     72\u001b[0m config \u001b[38;5;241m=\u001b[39m get_config()\n\u001b[1;32m     73\u001b[0m iterable_with_config \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m     74\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[1;32m     75\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[1;32m     76\u001b[0m )\n\u001b[0;32m---> 77\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterable_with_config\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/joblib/parallel.py:2072\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   2066\u001b[0m \u001b[38;5;66;03m# The first item from the output is blank, but it makes the interpreter\u001b[39;00m\n\u001b[1;32m   2067\u001b[0m \u001b[38;5;66;03m# progress until it enters the Try/Except block of the generator and\u001b[39;00m\n\u001b[1;32m   2068\u001b[0m \u001b[38;5;66;03m# reaches the first `yield` statement. This starts the asynchronous\u001b[39;00m\n\u001b[1;32m   2069\u001b[0m \u001b[38;5;66;03m# dispatch of the tasks to the workers.\u001b[39;00m\n\u001b[1;32m   2070\u001b[0m \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[0;32m-> 2072\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/joblib/parallel.py:1682\u001b[0m, in \u001b[0;36mParallel._get_outputs\u001b[0;34m(self, iterator, pre_dispatch)\u001b[0m\n\u001b[1;32m   1679\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m\n\u001b[1;32m   1681\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend\u001b[38;5;241m.\u001b[39mretrieval_context():\n\u001b[0;32m-> 1682\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_retrieve()\n\u001b[1;32m   1684\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mGeneratorExit\u001b[39;00m:\n\u001b[1;32m   1685\u001b[0m     \u001b[38;5;66;03m# The generator has been garbage collected before being fully\u001b[39;00m\n\u001b[1;32m   1686\u001b[0m     \u001b[38;5;66;03m# consumed. This aborts the remaining tasks if possible and warn\u001b[39;00m\n\u001b[1;32m   1687\u001b[0m     \u001b[38;5;66;03m# the user if necessary.\u001b[39;00m\n\u001b[1;32m   1688\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/joblib/parallel.py:1800\u001b[0m, in \u001b[0;36mParallel._retrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1789\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_ordered:\n\u001b[1;32m   1790\u001b[0m     \u001b[38;5;66;03m# Case ordered: wait for completion (or error) of the next job\u001b[39;00m\n\u001b[1;32m   1791\u001b[0m     \u001b[38;5;66;03m# that have been dispatched and not retrieved yet. If no job\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1795\u001b[0m     \u001b[38;5;66;03m# control only have to be done on the amount of time the next\u001b[39;00m\n\u001b[1;32m   1796\u001b[0m     \u001b[38;5;66;03m# dispatched job is pending.\u001b[39;00m\n\u001b[1;32m   1797\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (nb_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[1;32m   1798\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mget_status(timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtimeout) \u001b[38;5;241m==\u001b[39m TASK_PENDING\n\u001b[1;32m   1799\u001b[0m     ):\n\u001b[0;32m-> 1800\u001b[0m         \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0.01\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1801\u001b[0m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m   1803\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m nb_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m   1804\u001b[0m     \u001b[38;5;66;03m# Case unordered: jobs are added to the list of jobs to\u001b[39;00m\n\u001b[1;32m   1805\u001b[0m     \u001b[38;5;66;03m# retrieve `self._jobs` only once completed or in error, which\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1811\u001b[0m     \u001b[38;5;66;03m# timeouts before any other dispatched job has completed and\u001b[39;00m\n\u001b[1;32m   1812\u001b[0m     \u001b[38;5;66;03m# been added to `self._jobs` to be retrieved.\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "importlib.reload(train_flow)\n",
    "from itertools import combinations\n",
    "\n",
    "# --------- #\n",
    "# LEAVE ME  #\n",
    "# --------- #\n",
    "# Handling of new features and permutation importance\n",
    "pi_handlings = ['run_separately'] #'run_together', 'run_separately'\n",
    "train_flow.pi_handling_test(pi_handlings)\n",
    "keys = list(feature_dict.keys())\n",
    "two_combos = list(combinations(keys, 2))\n",
    "\n",
    "master_results = []\n",
    "master_perm = []\n",
    "\n",
    "\n",
    "for a, b in two_combos:\n",
    "# ---------- #\n",
    "# UPDATE ME  #\n",
    "# ---------- #\n",
    "    target_horizons = [1, 2, 5, 10, 20, 30]\n",
    "    train_years = [3]\n",
    "    pi_years = [1.5]\n",
    "    min_feats = [4]\n",
    "    days_assessed = 250\n",
    "    test_day = 5\n",
    "    feature_cols = feature_dict[a] + feature_dict[b]\n",
    "    list_name = f\"{a}-{b}\"\n",
    "    print(list_name)\n",
    "    groups = list_name.split(\"-\")\n",
    "\n",
    "\n",
    "    # --------- #\n",
    "    # LEAVE ME  #\n",
    "    # --------- # \n",
    "    models = {\"xgboost-tr3-d6-c1-lr:.1\": XGBClassifier(n_estimators=300, max_depth=6, colsample_bytree=1, min_child_weight=5, \n",
    "                                                        learning_rate=.1, random_state=42, n_jobs=-1)}\n",
    "    models = {\"xgboost-3\": XGBClassifier(n_estimators=300, random_state=42, n_jobs=-1)}\n",
    "    results_df = train_flow.run_train_flow(test_day, days_assessed, target_horizons, pi_handlings, feature_cols, df_main, models,\n",
    "                    train_years, pi_years, min_feats, list_name, feature_dict, groups)\n",
    "\n",
    "    master_results.append(results_df)\n",
    "    #master_perm.append(perm_df)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "for a in keys:\n",
    "\n",
    "    feature_cols = feature_dict[a] + feature_dict[b]\n",
    "    list_name = f\"{a}\"\n",
    "    print(list_name)\n",
    "    groups = list_name.split(\"-\")\n",
    "\n",
    "\n",
    "    # --------- #\n",
    "    # LEAVE ME  #\n",
    "    # --------- # \n",
    "    #models = {\"xgboost-3-f\": XGBClassifier(n_estimators=300, random_state=42, n_jobs=-1)}\n",
    "    models = {\"xgboost-1-10\": XGBClassifier(n_estimators=100, max_depth=10, colsample_bytree=0.75, min_child_weight=10, random_state=42, n_jobs=-1)}\n",
    "    models = {\"xgboost-tr3-d6-c1-lr:.1\": XGBClassifier(n_estimators=300, max_depth=6, colsample_bytree=1, min_child_weight=5, \n",
    "                                                        learning_rate=.1, random_state=42, n_jobs=-1)}\n",
    "    models = {\"xgboost-3\": XGBClassifier(n_estimators=300, min_child_weight=5, random_state=42, n_jobs=-1)}\n",
    "    results_df = train_flow.run_train_flow(test_day, days_assessed, target_horizons, pi_handlings, feature_cols, df_main, models,\n",
    "                    train_years, pi_years, min_feats, list_name, feature_dict, groups)\n",
    "\n",
    "    master_results.append(results_df)\n",
    "\"\"\"\n",
    "\n",
    "# final master DataFrames\n",
    "master_results_df = pd.concat(master_results, ignore_index=True)\n",
    "#master_results_df.to_csv('h5_performance.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fb8e9a0",
   "metadata": {},
   "source": [
    "# Save most recent run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "612f4f0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "rs = [1, 2, 5, 10, 20, 30]\n",
    "for r in target_horizons:\n",
    "    master_results_df[(master_results_df['test_days'] == 5) & (master_results_df['horizon'] == r)].to_csv(f\"h{r}_performance_all.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "id": "ea454c27",
   "metadata": {},
   "outputs": [],
   "source": [
    "master_results_df = pd.concat(master_results, ignore_index=True)\n",
    "base_df = pd.read_csv(\"h2_performance.csv\")\n",
    "cols = master_results_df.columns\n",
    "df_concat = pd.concat([base_df.drop_duplicates(), master_results_df[cols][master_results_df['horizon'] == 2].drop_duplicates()], ignore_index=True)\n",
    "df_concat.to_csv(\"h2_performance_all.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "617c564d",
   "metadata": {},
   "source": [
    "# Run Performance_Flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8787bbb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>horizon</th>\n",
       "      <th>model</th>\n",
       "      <th>train_years</th>\n",
       "      <th>feature_set</th>\n",
       "      <th>pi_size</th>\n",
       "      <th>min_feats</th>\n",
       "      <th>pos_rate</th>\n",
       "      <th>records</th>\n",
       "      <th>bal_prec</th>\n",
       "      <th>bal_prec_|0.75|</th>\n",
       "      <th>brier</th>\n",
       "      <th>log_loss</th>\n",
       "      <th>mcc</th>\n",
       "      <th>pprec</th>\n",
       "      <th>nprec</th>\n",
       "      <th>cov_|0.75|</th>\n",
       "      <th>pprec_|0.75|</th>\n",
       "      <th>nprec_|0.75|</th>\n",
       "      <th>composite</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>30</td>\n",
       "      <td>xgboost-3</td>\n",
       "      <td>5</td>\n",
       "      <td>ma_sma-experimental_slope</td>\n",
       "      <td>1.5-r</td>\n",
       "      <td>4</td>\n",
       "      <td>0.81</td>\n",
       "      <td>117.0</td>\n",
       "      <td>0.72</td>\n",
       "      <td>0.73</td>\n",
       "      <td>0.20</td>\n",
       "      <td>3.29</td>\n",
       "      <td>0.54</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.45</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.98</td>\n",
       "      <td>0.47</td>\n",
       "      <td>0.67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>30</td>\n",
       "      <td>xgboost-3</td>\n",
       "      <td>5</td>\n",
       "      <td>ma_lag-experimental_slope</td>\n",
       "      <td>1.5-r</td>\n",
       "      <td>4</td>\n",
       "      <td>0.83</td>\n",
       "      <td>116.0</td>\n",
       "      <td>0.71</td>\n",
       "      <td>0.69</td>\n",
       "      <td>0.19</td>\n",
       "      <td>4.13</td>\n",
       "      <td>0.53</td>\n",
       "      <td>0.97</td>\n",
       "      <td>0.45</td>\n",
       "      <td>0.94</td>\n",
       "      <td>0.97</td>\n",
       "      <td>0.42</td>\n",
       "      <td>0.65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>30</td>\n",
       "      <td>xgboost-3</td>\n",
       "      <td>5</td>\n",
       "      <td>ma_lag-ma_sma</td>\n",
       "      <td>1.5-r</td>\n",
       "      <td>4</td>\n",
       "      <td>0.83</td>\n",
       "      <td>116.0</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.71</td>\n",
       "      <td>0.18</td>\n",
       "      <td>4.63</td>\n",
       "      <td>0.51</td>\n",
       "      <td>0.96</td>\n",
       "      <td>0.45</td>\n",
       "      <td>0.96</td>\n",
       "      <td>0.96</td>\n",
       "      <td>0.46</td>\n",
       "      <td>0.65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>30</td>\n",
       "      <td>xgboost-3</td>\n",
       "      <td>5</td>\n",
       "      <td>ma_sma-vix_skew</td>\n",
       "      <td>1.5-r</td>\n",
       "      <td>4</td>\n",
       "      <td>0.82</td>\n",
       "      <td>113.0</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.72</td>\n",
       "      <td>0.19</td>\n",
       "      <td>1.98</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.97</td>\n",
       "      <td>0.42</td>\n",
       "      <td>0.89</td>\n",
       "      <td>0.97</td>\n",
       "      <td>0.47</td>\n",
       "      <td>0.64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>30</td>\n",
       "      <td>xgboost-3</td>\n",
       "      <td>5</td>\n",
       "      <td>ma_sma-pr</td>\n",
       "      <td>1.5-r</td>\n",
       "      <td>4</td>\n",
       "      <td>0.81</td>\n",
       "      <td>116.0</td>\n",
       "      <td>0.69</td>\n",
       "      <td>0.72</td>\n",
       "      <td>0.21</td>\n",
       "      <td>3.34</td>\n",
       "      <td>0.47</td>\n",
       "      <td>0.96</td>\n",
       "      <td>0.42</td>\n",
       "      <td>0.91</td>\n",
       "      <td>0.97</td>\n",
       "      <td>0.46</td>\n",
       "      <td>0.63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>30</td>\n",
       "      <td>xgboost-3</td>\n",
       "      <td>5</td>\n",
       "      <td>ma_num-rsi_macd</td>\n",
       "      <td>1.5-r</td>\n",
       "      <td>4</td>\n",
       "      <td>0.82</td>\n",
       "      <td>120.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.18</td>\n",
       "      <td>5.01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.82</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.82</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>30</td>\n",
       "      <td>xgboost-3</td>\n",
       "      <td>5</td>\n",
       "      <td>ma_sma-ma_num</td>\n",
       "      <td>1.5-r</td>\n",
       "      <td>4</td>\n",
       "      <td>0.82</td>\n",
       "      <td>120.0</td>\n",
       "      <td>0.58</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.17</td>\n",
       "      <td>4.17</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.82</td>\n",
       "      <td>0.33</td>\n",
       "      <td>0.97</td>\n",
       "      <td>0.82</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>30</td>\n",
       "      <td>xgboost-3</td>\n",
       "      <td>5</td>\n",
       "      <td>ma_num-pr</td>\n",
       "      <td>1.5-r</td>\n",
       "      <td>4</td>\n",
       "      <td>0.82</td>\n",
       "      <td>119.0</td>\n",
       "      <td>0.41</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.16</td>\n",
       "      <td>3.66</td>\n",
       "      <td>-0.04</td>\n",
       "      <td>0.81</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.97</td>\n",
       "      <td>0.83</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>30</td>\n",
       "      <td>xgboost-3</td>\n",
       "      <td>5</td>\n",
       "      <td>ma_num-volu</td>\n",
       "      <td>1.5-r</td>\n",
       "      <td>4</td>\n",
       "      <td>0.82</td>\n",
       "      <td>118.0</td>\n",
       "      <td>0.41</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.17</td>\n",
       "      <td>3.46</td>\n",
       "      <td>-0.04</td>\n",
       "      <td>0.82</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.97</td>\n",
       "      <td>0.82</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>30</td>\n",
       "      <td>xgboost-3</td>\n",
       "      <td>4</td>\n",
       "      <td>ma_num_ba</td>\n",
       "      <td>1.5</td>\n",
       "      <td>100</td>\n",
       "      <td>0.83</td>\n",
       "      <td>119.0</td>\n",
       "      <td>0.41</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.16</td>\n",
       "      <td>3.13</td>\n",
       "      <td>-0.06</td>\n",
       "      <td>0.83</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.96</td>\n",
       "      <td>0.83</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>107 rows × 19 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     horizon      model  train_years                feature_set pi_size  \\\n",
       "82        30  xgboost-3            5  ma_sma-experimental_slope   1.5-r   \n",
       "69        30  xgboost-3            5  ma_lag-experimental_slope   1.5-r   \n",
       "63        30  xgboost-3            5              ma_lag-ma_sma   1.5-r   \n",
       "81        30  xgboost-3            5            ma_sma-vix_skew   1.5-r   \n",
       "100       30  xgboost-3            5                  ma_sma-pr   1.5-r   \n",
       "..       ...        ...          ...                        ...     ...   \n",
       "83        30  xgboost-3            5            ma_num-rsi_macd   1.5-r   \n",
       "77        30  xgboost-3            5              ma_sma-ma_num   1.5-r   \n",
       "101       30  xgboost-3            5                  ma_num-pr   1.5-r   \n",
       "84        30  xgboost-3            5                ma_num-volu   1.5-r   \n",
       "26        30  xgboost-3            4                  ma_num_ba     1.5   \n",
       "\n",
       "     min_feats  pos_rate  records  bal_prec  bal_prec_|0.75|  brier  log_loss  \\\n",
       "82           4      0.81    117.0      0.72             0.73   0.20      3.29   \n",
       "69           4      0.83    116.0      0.71             0.69   0.19      4.13   \n",
       "63           4      0.83    116.0      0.70             0.71   0.18      4.63   \n",
       "81           4      0.82    113.0      0.70             0.72   0.19      1.98   \n",
       "100          4      0.81    116.0      0.69             0.72   0.21      3.34   \n",
       "..         ...       ...      ...       ...              ...    ...       ...   \n",
       "83           4      0.82    120.0       NaN              NaN   0.18      5.01   \n",
       "77           4      0.82    120.0      0.58              NaN   0.17      4.17   \n",
       "101          4      0.82    119.0      0.41              NaN   0.16      3.66   \n",
       "84           4      0.82    118.0      0.41              NaN   0.17      3.46   \n",
       "26         100      0.83    119.0      0.41              NaN   0.16      3.13   \n",
       "\n",
       "      mcc  pprec  nprec  cov_|0.75|  pprec_|0.75|  nprec_|0.75|  composite  \n",
       "82   0.54   0.99   0.45        0.95          0.98          0.47       0.67  \n",
       "69   0.53   0.97   0.45        0.94          0.97          0.42       0.65  \n",
       "63   0.51   0.96   0.45        0.96          0.96          0.46       0.65  \n",
       "81   0.50   0.97   0.42        0.89          0.97          0.47       0.64  \n",
       "100  0.47   0.96   0.42        0.91          0.97          0.46       0.63  \n",
       "..    ...    ...    ...         ...           ...           ...        ...  \n",
       "83    NaN   0.82    NaN        0.99          0.82           NaN        NaN  \n",
       "77   0.06   0.82   0.33        0.97          0.82           NaN        NaN  \n",
       "101 -0.04   0.81   0.00        0.97          0.83           NaN        NaN  \n",
       "84  -0.04   0.82   0.00        0.97          0.82           NaN        NaN  \n",
       "26  -0.06   0.83   0.00        0.96          0.83           NaN        NaN  \n",
       "\n",
       "[107 rows x 19 columns]"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import performance_flow\n",
    "importlib.reload(performance_flow)\n",
    "\n",
    "# ---------- #\n",
    "# UPDATE ME  #\n",
    "# ---------- #\n",
    "results_file_name = \"horizon_30_baseline.csv\" # Match prior cell saved as file name horizon_2_baseline_new\n",
    "min_th = 0.55\n",
    "cov_th = 0.75\n",
    "\n",
    "\n",
    "# --------- #\n",
    "# LEAVE ME  #\n",
    "# --------- #\n",
    "return_cols, perf_df = performance_flow.import_data(results_file_name, df_daily)\n",
    "composite_score = performance_flow.run_performance(perf_df, min_th, cov_th)\n",
    "bucket_df = performance_flow.bucket_scores(df_daily, perf_df, returns, min_th)\n",
    "cols = ['horizon', 'model', 'train_years', 'feature_set', 'pi_size',\n",
    "       'min_feats', 'test_days', 'pos_rate', 'bal_prec', 'pprec', 'nprec', 'records', 'pos_rate']\n",
    "\n",
    "composite_score[cols].head(6)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
