{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available Feature Sets: dict_keys(['ma', 'rsi', 'macd', 'volume', 'atr_adx', 'volatility', 'vix_skew', 'experimental_slope', 'past_return'])\n"
     ]
    }
   ],
   "source": [
    "import performance_flow\n",
    "import importlib\n",
    "importlib.reload(performance_flow)\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "import train_flow\n",
    "importlib.reload(train_flow)\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "# --------- #\n",
    "# LEAVE ME  #\n",
    "# --------- #\n",
    "ticker = 'QQQ'\n",
    "include_minute_feats = \"N\"\n",
    "returns = [1, 2, 3, 5, 10, 20, 30]\n",
    "df_daily, feature_sets, return_cols, daily_cols, feature_dict, features = train_flow.import_data(ticker, include_minute_feats, returns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Full Run\n",
    "- Retrain ALL models through most recent aod\n",
    "- Calculate performance\n",
    "- Select and save top n\n",
    "- Make predictions\n",
    "- Ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ma_lag-ma_rel already trained through most recent as_of_date\n",
      "ma_lag-ma_sma already trained through most recent as_of_date\n",
      "ma_lag-ma_num already trained through most recent as_of_date\n",
      "ma_lag-rsi_macd already trained through most recent as_of_date\n",
      "ma_lag-volu already trained through most recent as_of_date\n",
      "ma_lag-atr_adxvola already trained through most recent as_of_date\n",
      "ma_lag-vix_skew already trained through most recent as_of_date\n",
      "ma_lag-experimental_slope already trained through most recent as_of_date\n",
      "ma_lag-past_return already trained through most recent as_of_date\n",
      "ma_rel-ma_sma already trained through most recent as_of_date\n",
      "ma_rel-ma_num already trained through most recent as_of_date\n",
      "ma_rel-rsi_macd already trained through most recent as_of_date\n",
      "ma_rel-volu already trained through most recent as_of_date\n",
      "ma_rel-atr_adxvola already trained through most recent as_of_date\n",
      "ma_rel-vix_skew already trained through most recent as_of_date\n",
      "ma_rel-experimental_slope already trained through most recent as_of_date\n",
      "ma_rel-past_return already trained through most recent as_of_date\n",
      "ma_sma-ma_num already trained through most recent as_of_date\n",
      "ma_sma-rsi_macd already trained through most recent as_of_date\n",
      "ma_sma-volu already trained through most recent as_of_date\n",
      "ma_sma-atr_adxvola already trained through most recent as_of_date\n",
      "ma_sma-vix_skew already trained through most recent as_of_date\n",
      "ma_sma-experimental_slope already trained through most recent as_of_date\n",
      "ma_sma-past_return already trained through most recent as_of_date\n",
      "ma_num-rsi_macd already trained through most recent as_of_date\n",
      "ma_num-volu already trained through most recent as_of_date\n",
      "ma_num-atr_adxvola already trained through most recent as_of_date\n",
      "ma_num-vix_skew already trained through most recent as_of_date\n",
      "ma_num-experimental_slope already trained through most recent as_of_date\n",
      "ma_num-past_return already trained through most recent as_of_date\n",
      "rsi_macd-volu already trained through most recent as_of_date\n",
      "rsi_macd-atr_adxvola already trained through most recent as_of_date\n",
      "rsi_macd-vix_skew already trained through most recent as_of_date\n",
      "rsi_macd-experimental_slope already trained through most recent as_of_date\n",
      "rsi_macd-past_return already trained through most recent as_of_date\n",
      "volu-atr_adxvola already trained through most recent as_of_date\n",
      "volu-vix_skew already trained through most recent as_of_date\n",
      "volu-experimental_slope already trained through most recent as_of_date\n",
      "volu-past_return already trained through most recent as_of_date\n",
      "atr_adxvola-vix_skew already trained through most recent as_of_date\n",
      "atr_adxvola-experimental_slope already trained through most recent as_of_date\n",
      "atr_adxvola-past_return already trained through most recent as_of_date\n",
      "vix_skew-experimental_slope already trained through most recent as_of_date\n",
      "vix_skew-past_return already trained through most recent as_of_date\n",
      "experimental_slope-past_return already trained through most recent as_of_date\n",
      "Retrainig Done\n",
      "Horizon 2 Top 4 Models Saved\n",
      "2 | run_separately | rsi_macd-volu | 3 | 1.5 | 4 | 2\n",
      "Running new predictions for horizon 2 | run_separately\n",
      "Run 1/2 | Train: 2023-03-13 → 2026-02-13 | Test: 2026-02-19 → 2026-02-19 | Train_n=735 | Test_n=1 | (PI Years: 1.5 - Feats: 4)\n",
      "Run 2/2 | Train: 2023-03-10 → 2026-02-12 | Test: 2026-02-18 → 2026-02-18 | Train_n=735 | Test_n=1 | (PI Years: 1.5 - Feats: 4)\n",
      "Horizon 2 Top 4 Models Predicted\n",
      "2 | run_separately | rsi_macd-atr_adxvola | 3 | 1.5 | 4 | 2\n",
      "Running new predictions for horizon 2 | run_separately\n",
      "Run 1/2 | Train: 2023-03-13 → 2026-02-13 | Test: 2026-02-19 → 2026-02-19 | Train_n=735 | Test_n=1 | (PI Years: 1.5 - Feats: 4)\n",
      "Run 2/2 | Train: 2023-03-10 → 2026-02-12 | Test: 2026-02-18 → 2026-02-18 | Train_n=735 | Test_n=1 | (PI Years: 1.5 - Feats: 4)\n",
      "Horizon 2 Top 4 Models Predicted\n",
      "2 | run_separately | volu-past_return | 3 | 1.5 | 4 | 2\n",
      "Running new predictions for horizon 2 | run_separately\n",
      "Run 1/2 | Train: 2023-03-13 → 2026-02-13 | Test: 2026-02-19 → 2026-02-19 | Train_n=735 | Test_n=1 | (PI Years: 1.5 - Feats: 4)\n",
      "Run 2/2 | Train: 2023-03-10 → 2026-02-12 | Test: 2026-02-18 → 2026-02-18 | Train_n=735 | Test_n=1 | (PI Years: 1.5 - Feats: 4)\n",
      "Horizon 2 Top 4 Models Predicted\n",
      "2 | run_separately | atr_adxvola-past_return | 3 | 1.5 | 4 | 2\n",
      "Running new predictions for horizon 2 | run_separately\n",
      "Run 1/2 | Train: 2023-03-13 → 2026-02-13 | Test: 2026-02-19 → 2026-02-19 | Train_n=735 | Test_n=1 | (PI Years: 1.5 - Feats: 4)\n",
      "Run 2/2 | Train: 2023-03-10 → 2026-02-12 | Test: 2026-02-18 → 2026-02-18 | Train_n=735 | Test_n=1 | (PI Years: 1.5 - Feats: 4)\n",
      "Horizon 2 Top 4 Models Predicted\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>run</th>\n",
       "      <th>model</th>\n",
       "      <th>test_days</th>\n",
       "      <th>pred</th>\n",
       "      <th>train_n</th>\n",
       "      <th>train_start</th>\n",
       "      <th>train_end</th>\n",
       "      <th>test_start</th>\n",
       "      <th>test_end</th>\n",
       "      <th>train_years</th>\n",
       "      <th>n_features</th>\n",
       "      <th>pi_size</th>\n",
       "      <th>pi_handling</th>\n",
       "      <th>min_feats</th>\n",
       "      <th>features</th>\n",
       "      <th>horizon</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>xgboost-3</td>\n",
       "      <td>1</td>\n",
       "      <td>0.80</td>\n",
       "      <td>735</td>\n",
       "      <td>2023-03-10</td>\n",
       "      <td>2026-02-12</td>\n",
       "      <td>2026-02-18</td>\n",
       "      <td>2026-02-18</td>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>1.5</td>\n",
       "      <td>run_separately</td>\n",
       "      <td>4</td>\n",
       "      <td>rsi_macd-volu</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>xgboost-3</td>\n",
       "      <td>1</td>\n",
       "      <td>0.90</td>\n",
       "      <td>735</td>\n",
       "      <td>2023-03-10</td>\n",
       "      <td>2026-02-12</td>\n",
       "      <td>2026-02-18</td>\n",
       "      <td>2026-02-18</td>\n",
       "      <td>3</td>\n",
       "      <td>12</td>\n",
       "      <td>1.5</td>\n",
       "      <td>run_separately</td>\n",
       "      <td>4</td>\n",
       "      <td>rsi_macd-atr_adxvola</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2</td>\n",
       "      <td>xgboost-3</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00</td>\n",
       "      <td>735</td>\n",
       "      <td>2023-03-10</td>\n",
       "      <td>2026-02-12</td>\n",
       "      <td>2026-02-18</td>\n",
       "      <td>2026-02-18</td>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>1.5</td>\n",
       "      <td>run_separately</td>\n",
       "      <td>4</td>\n",
       "      <td>volu-past_return</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2</td>\n",
       "      <td>xgboost-3</td>\n",
       "      <td>1</td>\n",
       "      <td>0.55</td>\n",
       "      <td>735</td>\n",
       "      <td>2023-03-10</td>\n",
       "      <td>2026-02-12</td>\n",
       "      <td>2026-02-18</td>\n",
       "      <td>2026-02-18</td>\n",
       "      <td>3</td>\n",
       "      <td>12</td>\n",
       "      <td>1.5</td>\n",
       "      <td>run_separately</td>\n",
       "      <td>4</td>\n",
       "      <td>atr_adxvola-past_return</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   run      model  test_days  pred  train_n train_start   train_end  \\\n",
       "1    2  xgboost-3          1  0.80      735  2023-03-10  2026-02-12   \n",
       "3    2  xgboost-3          1  0.90      735  2023-03-10  2026-02-12   \n",
       "5    2  xgboost-3          1  0.00      735  2023-03-10  2026-02-12   \n",
       "7    2  xgboost-3          1  0.55      735  2023-03-10  2026-02-12   \n",
       "\n",
       "   test_start    test_end  train_years  n_features  pi_size     pi_handling  \\\n",
       "1  2026-02-18  2026-02-18            3          10      1.5  run_separately   \n",
       "3  2026-02-18  2026-02-18            3          12      1.5  run_separately   \n",
       "5  2026-02-18  2026-02-18            3          10      1.5  run_separately   \n",
       "7  2026-02-18  2026-02-18            3          12      1.5  run_separately   \n",
       "\n",
       "   min_feats                 features  horizon  \n",
       "1          4            rsi_macd-volu        2  \n",
       "3          4     rsi_macd-atr_adxvola        2  \n",
       "5          4         volu-past_return        2  \n",
       "7          4  atr_adxvola-past_return        2  "
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from itertools import chain\n",
    "import deployment_flow, performance_flow\n",
    "import importlib\n",
    "importlib.reload(deployment_flow)\n",
    "importlib.reload(performance_flow)\n",
    "\n",
    "def resolve_feature_cols(feature_set_name: str, features_dict: dict, sep: str = \"-\") -> list[str]:\n",
    "\n",
    "    # --- Kitchen sink case ---\n",
    "    if feature_set_name == \"kitch_sink\":\n",
    "        all_cols = chain.from_iterable(features_dict.values())\n",
    "        # dedupe preserve order\n",
    "        seen = set()\n",
    "        out = []\n",
    "        for c in all_cols:\n",
    "            if c not in seen:\n",
    "                seen.add(c)\n",
    "                out.append(c)\n",
    "        return out\n",
    "\n",
    "    # --- Normal composite case ---\n",
    "    parts = feature_set_name.split(sep)\n",
    "\n",
    "    cols = []\n",
    "    for p in parts:\n",
    "        if p not in features_dict:\n",
    "            raise KeyError(f\"{p} not in features_dict\")\n",
    "        cols.append(features_dict[p])\n",
    "\n",
    "    # flatten + dedupe\n",
    "    seen = set()\n",
    "    out = []\n",
    "    for c in chain.from_iterable(cols):\n",
    "        if c not in seen:\n",
    "            seen.add(c)\n",
    "            out.append(c)\n",
    "\n",
    "    return out\n",
    "\n",
    "h=[2]\n",
    "master_results = []\n",
    "master_preds = []\n",
    "n = 4 # number of top models to select \n",
    "file_ext = \"performance_all\"\n",
    "min_th = 0.55\n",
    "cov_th = 0.75\n",
    "\n",
    "# Retrain ALL\n",
    "for r in h:\n",
    "    \n",
    "    df = pd.read_csv(f\"h{r}_{file_ext}.csv\")\n",
    "    df = df.dropna().copy()\n",
    "    #df = df.rename(columns={\"feature_set\": \"features\"})\n",
    "\n",
    "    df[\"feature_cols\"] = df[\"features\"].apply(lambda x: resolve_feature_cols(x, feature_dict))\n",
    "\n",
    "    grain_cols = [\"horizon\",\"features\",\"train_years\",\"min_feats\",\"pi_size\",\"model\",\"pi_handling\"]\n",
    "\n",
    "    max_train = (\n",
    "        df.groupby(grain_cols, as_index=False)[\"test_start\"]\n",
    "        .max()\n",
    "        .rename(columns={\"test_start\": \"max_test_start\"})\n",
    "    )\n",
    "\n",
    "    models = (\n",
    "        df[grain_cols].drop_duplicates(subset=grain_cols, keep=\"first\")\n",
    "        .merge(df[grain_cols + [\"feature_cols\"]].drop_duplicates(subset=grain_cols), on=grain_cols, how=\"left\")\n",
    "        .merge(max_train, on=grain_cols, how=\"left\")   # <-- this is the missing piece\n",
    "    )\n",
    "    \n",
    "    for row in models.itertuples(index=False):\n",
    "\n",
    "        target_horizon = row.horizon\n",
    "        pi_handling    = 'run_separately' #row.pi_handling\n",
    "        type           = 'Actualized'\n",
    "        feature_cols   = row.feature_cols   # list-of-cols wrapped in a list\n",
    "        list_name      = row.features\n",
    "        train_year     = row.train_years\n",
    "        pi_year        = row.pi_size\n",
    "        min_feat       = row.min_feats\n",
    "        max_test_start = row.max_test_start\n",
    "        days_assessed  = len(df_daily.iloc[r:][df_daily['Date'] > max_test_start].copy())\n",
    "        groups = list_name.split(\"-\")\n",
    "\n",
    "        if days_assessed > 0:\n",
    "\n",
    "            model = XGBClassifier(n_estimators=300, random_state=42, n_jobs=-1)\n",
    "            model_name = \"xgboost-3\"\n",
    "\n",
    "            print(f\"{target_horizon} | {pi_handling} | {list_name} | {train_year} | {pi_year} | {min_feat} | {days_assessed}\")\n",
    "            results_df = deployment_flow.run_deploy_flow(days_assessed, r, pi_handling, feature_cols, df_daily, model_name, model,\n",
    "                            train_year, pi_year, min_feat, list_name, feature_dict, groups, type)\n",
    "            \n",
    "            master_results.append(results_df)\n",
    "\n",
    "        else:\n",
    "\n",
    "            print(f\"{list_name} already trained through most recent as_of_date\")\n",
    "    \n",
    "    print(f\"Retrainig Done\")\n",
    "    if len(master_results) > 0: \n",
    "        \n",
    "        master_results_df = pd.concat(master_results, ignore_index=True)\n",
    "        performance_df = pd.read_csv(f\"h{r}_{file_ext}.csv\")\n",
    "        df_concat = pd.concat([performance_df, master_results_df], ignore_index=True)    \n",
    "        df_concat.to_csv(f\"h{r}_{file_ext}.csv\", index=False)\n",
    "\n",
    "# Performance and Top n\n",
    "for r in h:\n",
    "\n",
    "    keys = [\"horizon\", \"features\", \"train_years\", \"min_feats\", \"pi_size\", \"pi_handling\", \"model\"]\n",
    "\n",
    "    results_file_name = f\"h{r}_{file_ext}.csv\" # Match prior cell saved as file name horizon_2_baseline_new\n",
    "    return_cols, perf_df = performance_flow.import_data(results_file_name, df_daily)\n",
    "    perf_df = perf_df.rename(columns={\"feature_set\": \"features\"})\n",
    "    composite_score = performance_flow.run_performance(perf_df[perf_df['horizon'] == r].dropna(), min_th, cov_th)\n",
    "    bucket_df = performance_flow.bucket_scores(df_daily.dropna(), perf_df[perf_df['horizon'] == r].dropna(), returns, min_th, keys)\n",
    "\n",
    "    top_n = (\n",
    "    composite_score.sort_values(\"composite\", ascending=False)\n",
    "    .drop_duplicates(subset=[\"features\"], keep=\"first\").head(n).copy())\n",
    "\n",
    "    # Ensure dtypes match so the join actually hits\n",
    "    for df in (top_n, perf_df):\n",
    "\n",
    "        df[\"horizon\"] = r\n",
    "        df[\"features\"] = df[\"features\"].astype(str)\n",
    "        df[\"model\"]       = df[\"model\"].astype(str)\n",
    "        df[\"pi_size\"]     = df[\"pi_size\"]\n",
    "        df[\"pi_handling\"]     = df[\"pi_handling\"].astype(str)\n",
    "        df[\"train_years\"] = df[\"train_years\"].astype(int)\n",
    "        df[\"min_feats\"]   = df[\"min_feats\"].astype(int)\n",
    "\n",
    "    # Filter master predictions to only rows matching one of the 10 configs\n",
    "    pred_filtered = perf_df.merge(top_n[keys].drop_duplicates(), on=keys, how=\"inner\")\n",
    "    #print(len(pred_filtered))\n",
    "    pred_filtered.to_csv(f\"h{r}_top{n}_{file_ext}.csv\", index=False)\n",
    "    print(f\"Horizon {r} Top {n} Models Saved\")\n",
    "\n",
    "# Predictions Top n\n",
    "for r in h:\n",
    "    \n",
    "    days_assessed = len(df_daily[df_daily[f\"Return_{r}\"].isna()])\n",
    "\n",
    "    df = pd.read_csv(f\"h{r}_top{n}_{file_ext}.csv\")\n",
    "\n",
    "    df[\"feature_cols\"] = df[\"features\"].apply(lambda x: resolve_feature_cols(x, feature_dict))\n",
    "\n",
    "    grain_cols = [\"horizon\",\"features\",\"train_years\",\"min_feats\",\"pi_size\",\"model\",\"pi_handling\"]\n",
    "\n",
    "    top_n = (\n",
    "        df[grain_cols].drop_duplicates(subset=grain_cols, keep=\"first\")\n",
    "        .merge(df[grain_cols + [\"feature_cols\"]].drop_duplicates(subset=grain_cols), on=grain_cols, how=\"left\"))\n",
    "    \n",
    "    for row in top_n.itertuples(index=False):\n",
    "\n",
    "        target_horizon = row.horizon\n",
    "        pi_handling    = 'run_separately' #row.pi_handling\n",
    "        type           = 'New_Predict'\n",
    "        feature_cols   = row.feature_cols   # list-of-cols wrapped in a list\n",
    "        list_name      = row.features\n",
    "        train_year     = row.train_years\n",
    "        pi_year        = row.pi_size\n",
    "        min_feat       = row.min_feats\n",
    "        groups = list_name.split(\"-\")\n",
    "\n",
    "        model = XGBClassifier(n_estimators=300, random_state=42, n_jobs=-1)\n",
    "        model_name = \"xgboost-3\"\n",
    "\n",
    "        print(f\"{target_horizon} | {pi_handling} | {list_name} | {train_year} | {pi_year} | {min_feat} | {days_assessed}\")\n",
    "        results_df = deployment_flow.run_deploy_flow(days_assessed, r, pi_handling, feature_cols, df_daily, model_name, model,\n",
    "                        train_year, pi_year, min_feat, list_name, feature_dict, groups, type)\n",
    "        \n",
    "        master_preds.append(results_df)\n",
    "        print(f\"Horizon {r} Top {n} Models Predicted\")\n",
    "\n",
    "    master_preds_df = pd.concat(master_preds, ignore_index=True)\n",
    "    predictions_df = master_preds_df.copy()\n",
    "    composite_score[['pprec', 'nprec'] + keys].drop_duplicates().merge(predictions_df, on=keys, how=\"inner\")\n",
    "\n",
    "predictions_df.sort_values(by='test_start', ascending=False).head(n)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>features</th>\n",
       "      <th>pred</th>\n",
       "      <th>pprec</th>\n",
       "      <th>nprec</th>\n",
       "      <th>Predicted_Price</th>\n",
       "      <th>Last_Close</th>\n",
       "      <th>LC_R_PP</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2026-02-18</td>\n",
       "      <td>rsi_macd-volu</td>\n",
       "      <td>0.80</td>\n",
       "      <td>0.62</td>\n",
       "      <td>0.50</td>\n",
       "      <td>605.79</td>\n",
       "      <td>603.47</td>\n",
       "      <td>-0.004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2026-02-18</td>\n",
       "      <td>volu-past_return</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.49</td>\n",
       "      <td>605.79</td>\n",
       "      <td>603.47</td>\n",
       "      <td>-0.004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2026-02-18</td>\n",
       "      <td>rsi_macd-atr_adxvola</td>\n",
       "      <td>0.90</td>\n",
       "      <td>0.60</td>\n",
       "      <td>0.49</td>\n",
       "      <td>605.79</td>\n",
       "      <td>603.47</td>\n",
       "      <td>-0.004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2026-02-18</td>\n",
       "      <td>ma_sma-volu</td>\n",
       "      <td>0.65</td>\n",
       "      <td>0.59</td>\n",
       "      <td>0.48</td>\n",
       "      <td>605.79</td>\n",
       "      <td>603.47</td>\n",
       "      <td>-0.004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2026-02-19</td>\n",
       "      <td>rsi_macd-volu</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.62</td>\n",
       "      <td>0.50</td>\n",
       "      <td>603.47</td>\n",
       "      <td>603.47</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2026-02-19</td>\n",
       "      <td>volu-past_return</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.49</td>\n",
       "      <td>603.47</td>\n",
       "      <td>603.47</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2026-02-19</td>\n",
       "      <td>rsi_macd-atr_adxvola</td>\n",
       "      <td>0.65</td>\n",
       "      <td>0.60</td>\n",
       "      <td>0.49</td>\n",
       "      <td>603.47</td>\n",
       "      <td>603.47</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2026-02-19</td>\n",
       "      <td>ma_sma-volu</td>\n",
       "      <td>0.45</td>\n",
       "      <td>0.59</td>\n",
       "      <td>0.48</td>\n",
       "      <td>603.47</td>\n",
       "      <td>603.47</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Date              features  pred  pprec  nprec  Predicted_Price  \\\n",
       "0  2026-02-18         rsi_macd-volu  0.80   0.62   0.50           605.79   \n",
       "1  2026-02-18      volu-past_return  0.00   0.63   0.49           605.79   \n",
       "2  2026-02-18  rsi_macd-atr_adxvola  0.90   0.60   0.49           605.79   \n",
       "3  2026-02-18           ma_sma-volu  0.65   0.59   0.48           605.79   \n",
       "4  2026-02-19         rsi_macd-volu  0.95   0.62   0.50           603.47   \n",
       "5  2026-02-19      volu-past_return  0.10   0.63   0.49           603.47   \n",
       "6  2026-02-19  rsi_macd-atr_adxvola  0.65   0.60   0.49           603.47   \n",
       "7  2026-02-19           ma_sma-volu  0.45   0.59   0.48           603.47   \n",
       "\n",
       "   Last_Close  LC_R_PP  \n",
       "0      603.47   -0.004  \n",
       "1      603.47   -0.004  \n",
       "2      603.47   -0.004  \n",
       "3      603.47   -0.004  \n",
       "4      603.47    0.000  \n",
       "5      603.47    0.000  \n",
       "6      603.47    0.000  \n",
       "7      603.47    0.000  "
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_df = composite_score[['pprec', 'nprec'] + keys].drop_duplicates().merge(predictions_df, on=keys, how=\"inner\")\n",
    "output_df = output_df.rename(columns={\"test_start\": \"Date\"})\n",
    "cols = ['Date', 'features', 'pred', 'pprec', 'nprec']\n",
    "output_df = output_df[cols].sort_values(by='Date').copy()\n",
    "output_df = output_df.merge(df_daily[['Close', 'Date']].round(2), on='Date', how=\"inner\")\n",
    "output_df = output_df.rename(columns={\"Close\": \"Predicted_Price\"})\n",
    "last_close = (df_daily.sort_values(\"Date\", ascending=False).iloc[0][\"Close\"].round(2))\n",
    "output_df['Last_Close'] = last_close\n",
    "output_df['LC_R_PP'] = round(output_df['Last_Close'] / output_df['Predicted_Price'] - 1, 3)\n",
    "\n",
    "output_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parse Top Models per Horizon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "importlib.reload(performance_flow)\n",
    "# ---------- #\n",
    "# UPDATE ME  #\n",
    "# ---------- #\n",
    "min_th = 0.55\n",
    "cov_th = 0.75\n",
    "horizons = [2]#[2, 5, 10, 20, 30]\n",
    "\n",
    "for r in horizons:\n",
    "    \n",
    "    results_file_name = f\"h{r}_performance.csv\" # Match prior cell saved as file name horizon_2_baseline_new\n",
    "    return_cols, perf_df = performance_flow.import_data(results_file_name, df_daily)\n",
    "    perf_df = perf_df.rename(columns={\"feature_set\": \"features\"})\n",
    "    composite_score = performance_flow.run_performance(perf_df[perf_df['horizon'] == r], min_th, cov_th)\n",
    "    #bucket_df = performance_flow.bucket_scores(df_daily, perf_df[perf_df['horizon'] == r], returns, min_th, keys)\n",
    "\n",
    "    top_10_r = (\n",
    "    composite_score.sort_values(\"composite\", ascending=False)\n",
    "    .drop_duplicates(subset=[\"features\"], keep=\"first\").head(5).copy())\n",
    "\n",
    "    keys = [\"horizon\", \"features\", \"train_years\", \"min_feats\", \"pi_size\", \"pi_handling\", \"model\"]\n",
    "\n",
    "    # Ensure dtypes match so the join actually hits\n",
    "    for df in (top_10_r, perf_df):\n",
    "\n",
    "        df[\"horizon\"] = r\n",
    "        df[\"features\"] = df[\"features\"].astype(str)\n",
    "        df[\"model\"]       = df[\"model\"].astype(str)\n",
    "        df[\"pi_size\"]     = df[\"pi_size\"]\n",
    "        df[\"pi_handling\"]     = df[\"pi_handling\"].astype(str)\n",
    "        df[\"train_years\"] = df[\"train_years\"].astype(int)\n",
    "        df[\"min_feats\"]   = df[\"min_feats\"].astype(int)\n",
    "\n",
    "    # Filter master predictions to only rows matching one of the 10 configs\n",
    "    pred_filtered = perf_df.merge(top_10_r[keys].drop_duplicates(), on=keys, how=\"inner\")\n",
    "    #print(len(pred_filtered))\n",
    "    pred_filtered.to_csv(f\"h{r}_top10_raw.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Top Performers through most recent as_of_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ma_lag-past_return already trained through most recent as_of_date\n",
      "ma_rel-past_return already trained through most recent as_of_date\n",
      "ma_sma-past_return already trained through most recent as_of_date\n",
      "ma_num-past_return already trained through most recent as_of_date\n",
      "rsi_macd-past_return already trained through most recent as_of_date\n",
      "volu-past_return already trained through most recent as_of_date\n",
      "atr_adxvola-vix_skew already trained through most recent as_of_date\n",
      "atr_adxvola-past_return already trained through most recent as_of_date\n",
      "vix_skew-past_return already trained through most recent as_of_date\n",
      "experimental_slope-past_return already trained through most recent as_of_date\n"
     ]
    }
   ],
   "source": [
    "from itertools import chain\n",
    "import deployment_flow\n",
    "import importlib\n",
    "importlib.reload(deployment_flow)\n",
    "\n",
    "def resolve_feature_cols(feature_set_name: str, features_dict: dict, sep: str = \"-\") -> list[str]:\n",
    "\n",
    "    # --- Kitchen sink case ---\n",
    "    if feature_set_name == \"kitch_sink\":\n",
    "        all_cols = chain.from_iterable(features_dict.values())\n",
    "        # dedupe preserve order\n",
    "        seen = set()\n",
    "        out = []\n",
    "        for c in all_cols:\n",
    "            if c not in seen:\n",
    "                seen.add(c)\n",
    "                out.append(c)\n",
    "        return out\n",
    "\n",
    "    # --- Normal composite case ---\n",
    "    parts = feature_set_name.split(sep)\n",
    "\n",
    "    cols = []\n",
    "    for p in parts:\n",
    "        if p not in features_dict:\n",
    "            raise KeyError(f\"{p} not in features_dict\")\n",
    "        cols.append(features_dict[p])\n",
    "\n",
    "    # flatten + dedupe\n",
    "    seen = set()\n",
    "    out = []\n",
    "    for c in chain.from_iterable(cols):\n",
    "        if c not in seen:\n",
    "            seen.add(c)\n",
    "            out.append(c)\n",
    "\n",
    "    return out\n",
    "\n",
    "h=[2]\n",
    "master_results = []\n",
    "\n",
    "for r in h:\n",
    "    \n",
    "    df = pd.read_csv(f\"h{r}_top10_raw.csv\")\n",
    "    df = df.rename(columns={\"feature_set\": \"features\"})\n",
    "\n",
    "    df[\"feature_cols\"] = df[\"features\"].apply(lambda x: resolve_feature_cols(x, feature_dict))\n",
    "\n",
    "    grain_cols = [\"horizon\",\"features\",\"train_years\",\"min_feats\",\"pi_size\",\"model\",\"pi_handling\"]\n",
    "\n",
    "    max_train = (\n",
    "        df.groupby(grain_cols, as_index=False)[\"test_start\"]\n",
    "        .max()\n",
    "        .rename(columns={\"test_start\": \"max_test_start\"})\n",
    "    )\n",
    "\n",
    "    top_10_unique = (\n",
    "        df[grain_cols].drop_duplicates(subset=grain_cols, keep=\"first\")\n",
    "        .merge(df[grain_cols + [\"feature_cols\"]].drop_duplicates(subset=grain_cols), on=grain_cols, how=\"left\")\n",
    "        .merge(max_train, on=grain_cols, how=\"left\")   # <-- this is the missing piece\n",
    "    )\n",
    "    \n",
    "    for row in top_10_unique.itertuples(index=False):\n",
    "\n",
    "        target_horizon = row.horizon\n",
    "        pi_handling    = 'run_separately' #row.pi_handling\n",
    "        feature_cols   = row.feature_cols   # list-of-cols wrapped in a list\n",
    "        list_name      = row.features\n",
    "        train_year     = row.train_years\n",
    "        pi_year        = row.pi_size\n",
    "        min_feat       = row.min_feats\n",
    "        max_test_start = row.max_test_start\n",
    "        days_assessed  = len(df_daily.iloc[r:][df_daily['Date'] > max_test_start].copy())\n",
    "        groups = list_name.split(\"-\")\n",
    "\n",
    "        if days_assessed > 0:\n",
    "\n",
    "            model = XGBClassifier(n_estimators=300, random_state=42, n_jobs=-1)\n",
    "            model_name = \"xgboost-3\"\n",
    "\n",
    "            print(f\"{target_horizon} | {pi_handling} | {list_name} | {train_year} | {pi_year} | {min_feat} | {days_assessed}\")\n",
    "            results_df = deployment_flow.run_deploy_flow(days_assessed, r, pi_handling, feature_cols, df_daily, model_name, model,\n",
    "                            train_year, pi_year, min_feat, list_name, feature_dict, groups)\n",
    "            \n",
    "            master_results.append(results_df)\n",
    "\n",
    "        else:\n",
    "\n",
    "            1\n",
    "            #print(f\"{list_name} already trained through most recent as_of_date\")\n",
    "\n",
    "if len(master_results) > 0: \n",
    "    master_results_df = pd.concat(master_results, ignore_index=True)\n",
    "\n",
    "#performance_df = pd.read_csv(f\"h{r}_baseline.csv\")\n",
    "#df_concat = pd.concat([performance_df, results_df], ignore_index=True)    \n",
    "#df_concat.to_csv(f\"h{r}_baseline_v2.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Train through most recent actualized data (Done)\n",
    "- Append all actuals to raw files\n",
    "- Recalc performance \n",
    "- Predict on new horizon (Done)\n",
    "- Show prediction (Done)\n",
    "- Bring in performance\n",
    "- Ensemble prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rerun Performance with updated as_of_dates \n",
    "- (to be merged with prior step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "importlib.reload(performance_flow)\n",
    "# ---------- #\n",
    "# UPDATE ME  #\n",
    "# ---------- #\n",
    "min_th = 0.55\n",
    "cov_th = 0.75\n",
    "horizons = [2]#[2, 5, 10, 20, 30]\n",
    "\n",
    "for r in horizons:\n",
    "    \n",
    "    keys = [\"horizon\", \"features\", \"train_years\", \"min_feats\", \"pi_size\", \"pi_handling\", \"model\"]\n",
    "\n",
    "    results_file_name = f\"h{r}_{file_ext}.csv\" # Match prior cell saved as file name horizon_2_baseline_new\n",
    "    return_cols, perf_df = performance_flow.import_data(results_file_name, df_daily)\n",
    "    perf_df = perf_df.rename(columns={\"feature_set\": \"features\"})\n",
    "    composite_score = performance_flow.run_performance(perf_df[perf_df['horizon'] == r].dropna(), min_th, cov_th)\n",
    "    bucket_df = performance_flow.bucket_scores(df_daily.dropna(), perf_df[perf_df['horizon'] == r].dropna(), returns, min_th, keys)\n",
    "\n",
    "    top_10_r = (\n",
    "    composite_score.sort_values(\"composite\", ascending=False)\n",
    "    .drop_duplicates(subset=[\"features\"], keep=\"first\").head(10).copy())\n",
    "\n",
    "    # Ensure dtypes match so the join actually hits\n",
    "    for df in (top_10_r, perf_df):\n",
    "\n",
    "        df[\"horizon\"] = r\n",
    "        df[\"features\"] = df[\"features\"].astype(str)\n",
    "        df[\"model\"]       = df[\"model\"].astype(str)\n",
    "        df[\"pi_size\"]     = df[\"pi_size\"]\n",
    "        df[\"pi_handling\"]     = df[\"pi_handling\"].astype(str)\n",
    "        df[\"train_years\"] = df[\"train_years\"].astype(int)\n",
    "        df[\"min_feats\"]   = df[\"min_feats\"].astype(int)\n",
    "\n",
    "    # Filter master predictions to only rows matching one of the 10 configs\n",
    "    #pred_filtered = perf_df.merge(top_10_r[keys].drop_duplicates(), on=keys, how=\"inner\")\n",
    "    #print(len(pred_filtered))\n",
    "    #pred_filtered.to_csv(f\"h{r}_top10_raw.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predictions for un-actualized records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 | run_separately | ma_lag-vix_skew | 3 | 1.5 | 4 | 2\n",
      "Running new predictions for horizon 2 | run_separately\n",
      "ma_lag: 60 | 5 | ['SMA_10_Lag100_min', 'SMA_10_Lag10_max', 'SMA_10_Lag10_min', 'SMA_10_Lag150_min', 'SMA_10_Lag50_min']\n",
      "vix_skew: 12 | 5 | ['VIX_10_change', 'skew', 'skew_10_change', 'skew_5_change', 'skew_rolling_std']\n",
      "Run 1/2 | Train: 2023-03-08 → 2026-02-10 | Test: 2026-02-13 → 2026-02-13 | Train_n=735 | Test_n=1 | (PI Years: 1.5 - Feats: 4)\n",
      "ma_lag: 60 | 5 | ['SMA_10_Lag100_min', 'SMA_10_Lag10_max', 'SMA_10_Lag10_min', 'SMA_10_Lag50_min', 'SMA_50_Lag150_min']\n",
      "vix_skew: 12 | 5 | ['VIX_10_change', 'VIX_1_change', 'skew', 'skew_10_change', 'skew_rolling_std']\n",
      "Run 2/2 | Train: 2023-03-07 → 2026-02-09 | Test: 2026-02-12 → 2026-02-12 | Train_n=735 | Test_n=1 | (PI Years: 1.5 - Feats: 4)\n",
      "2 | run_separately | ma_sma-volu | 3 | 1.5 | 4 | 2\n",
      "Running new predictions for horizon 2 | run_separately\n",
      "ma_sma: 15 | 11 | ['10_SMA_200', '10_SMA_25', '10_SMA_50', '25_SMA_100', '25_SMA_200', '50_SMA_100', '50_SMA_200', 'SMA_10', 'SMA_200', 'SMA_25', 'SMA_50']\n",
      "volu: 26 | 4 | ['CMF_20', 'OBV', 'Vol_Ratio_100_zscore', 'Vol_Ratio_50_zscore']\n",
      "Run 1/2 | Train: 2023-03-08 → 2026-02-10 | Test: 2026-02-13 → 2026-02-13 | Train_n=735 | Test_n=1 | (PI Years: 1.5 - Feats: 4)\n",
      "ma_sma: 15 | 11 | ['10_SMA_100', '10_SMA_200', '10_SMA_25', '10_SMA_50', '25_SMA_100', '25_SMA_200', '50_SMA_200', 'SMA_10', 'SMA_200', 'SMA_25', 'SMA_50']\n",
      "volu: 26 | 4 | ['OBV', 'Vol_Ratio_100', 'Vol_Ratio_100_zscore', 'Vol_Ratio_50_zscore']\n",
      "Run 2/2 | Train: 2023-03-07 → 2026-02-09 | Test: 2026-02-12 → 2026-02-12 | Train_n=735 | Test_n=1 | (PI Years: 1.5 - Feats: 4)\n",
      "2 | run_separately | ma_num-vix_skew | 3 | 1.5 | 4 | 2\n",
      "Running new predictions for horizon 2 | run_separately\n",
      "ma_num: 13 | 8 | ['Min_10_Rows_Since', 'Min_120_Rows_Since', 'Min_240_Rows_Since', 'Min_30_Rows_Since', 'Min_60_Rows_Since', 'num_days_100', 'num_days_200', 'num_days_50']\n",
      "vix_skew: 12 | 5 | ['VIX_10_change', 'skew', 'skew_10_change', 'skew_5_change', 'skew_rolling_std']\n",
      "Run 1/2 | Train: 2023-03-08 → 2026-02-10 | Test: 2026-02-13 → 2026-02-13 | Train_n=735 | Test_n=1 | (PI Years: 1.5 - Feats: 4)\n",
      "ma_num: 13 | 8 | ['Min_10_Rows_Since', 'Min_120_Rows_Since', 'Min_240_Rows_Since', 'Min_30_Rows_Since', 'Min_60_Rows_Since', 'num_days_100', 'num_days_200', 'num_days_50']\n",
      "vix_skew: 12 | 5 | ['VIX_10_change', 'VIX_1_change', 'skew', 'skew_10_change', 'skew_rolling_std']\n",
      "Run 2/2 | Train: 2023-03-07 → 2026-02-09 | Test: 2026-02-12 → 2026-02-12 | Train_n=735 | Test_n=1 | (PI Years: 1.5 - Feats: 4)\n",
      "2 | run_separately | volu-past_return | 3 | 1.5 | 4 | 2\n",
      "Running new predictions for horizon 2 | run_separately\n",
      "volu: 26 | 4 | ['CMF_20', 'OBV', 'Vol_Ratio_100_zscore', 'Vol_Ratio_50_zscore']\n",
      "past_return: 14 | 6 | ['Past_Return%_1', 'Past_Return%_10', 'Past_Return%_20', 'Past_Return%_3', 'Past_Return%_30', 'Past_Return%_5']\n",
      "Run 1/2 | Train: 2023-03-08 → 2026-02-10 | Test: 2026-02-13 → 2026-02-13 | Train_n=735 | Test_n=1 | (PI Years: 1.5 - Feats: 4)\n",
      "volu: 26 | 4 | ['OBV', 'Vol_Ratio_100', 'Vol_Ratio_100_zscore', 'Vol_Ratio_50_zscore']\n",
      "past_return: 14 | 6 | ['Past_Return%_1', 'Past_Return%_10', 'Past_Return%_20', 'Past_Return%_3', 'Past_Return%_30', 'Past_Return%_5']\n",
      "Run 2/2 | Train: 2023-03-07 → 2026-02-09 | Test: 2026-02-12 → 2026-02-12 | Train_n=735 | Test_n=1 | (PI Years: 1.5 - Feats: 4)\n",
      "2 | run_separately | atr_adxvola-past_return | 3 | 1.5 | 4 | 2\n",
      "Running new predictions for horizon 2 | run_separately\n",
      "atr_adxvola: 12 | 7 | ['ADX', 'ATR_14', 'ATR_21', 'Price_Vol_Ratio_25', 'Price_Vol_Ratio_5', 'minus_DI', 'vol_5']\n",
      "past_return: 14 | 6 | ['Past_Return%_1', 'Past_Return%_10', 'Past_Return%_20', 'Past_Return%_3', 'Past_Return%_30', 'Past_Return%_5']\n",
      "Run 1/2 | Train: 2023-03-08 → 2026-02-10 | Test: 2026-02-13 → 2026-02-13 | Train_n=735 | Test_n=1 | (PI Years: 1.5 - Feats: 4)\n",
      "atr_adxvola: 12 | 6 | ['ADX', 'ATR_14', 'ATR_21', 'Price_Vol_Ratio_25', 'Price_Vol_Ratio_5', 'minus_DI']\n",
      "past_return: 14 | 6 | ['Past_Return%_1', 'Past_Return%_10', 'Past_Return%_20', 'Past_Return%_3', 'Past_Return%_30', 'Past_Return%_5']\n",
      "Run 2/2 | Train: 2023-03-07 → 2026-02-09 | Test: 2026-02-12 → 2026-02-12 | Train_n=735 | Test_n=1 | (PI Years: 1.5 - Feats: 4)\n"
     ]
    }
   ],
   "source": [
    "import deployment_flow\n",
    "import importlib\n",
    "importlib.reload(deployment_flow)\n",
    "\n",
    "h = [2]\n",
    "master_results = []\n",
    "\n",
    "for r in h:\n",
    "    \n",
    "    days_assessed = len(df_daily[df_daily[f\"Return_{r}\"].isna()])\n",
    "\n",
    "    df = pd.read_csv(f\"h{r}_top10_raw.csv\")\n",
    "\n",
    "    df[\"feature_cols\"] = df[\"features\"].apply(lambda x: resolve_feature_cols(x, feature_dict))\n",
    "\n",
    "    grain_cols = [\"horizon\",\"features\",\"train_years\",\"min_feats\",\"pi_size\",\"model\",\"pi_handling\"]\n",
    "\n",
    "    top_10_unique = (\n",
    "        df[grain_cols].drop_duplicates(subset=grain_cols, keep=\"first\")\n",
    "        .merge(df[grain_cols + [\"feature_cols\"]].drop_duplicates(subset=grain_cols), on=grain_cols, how=\"left\"))\n",
    "    \n",
    "    for row in top_10_unique.itertuples(index=False):\n",
    "\n",
    "        target_horizon = row.horizon\n",
    "        pi_handling    = 'run_separately' #row.pi_handling\n",
    "        type           = 'New_Predict'\n",
    "        feature_cols   = row.feature_cols   # list-of-cols wrapped in a list\n",
    "        list_name      = row.features\n",
    "        train_year     = row.train_years\n",
    "        pi_year        = row.pi_size\n",
    "        min_feat       = row.min_feats\n",
    "        groups = list_name.split(\"-\")\n",
    "\n",
    "        model = XGBClassifier(n_estimators=300, random_state=42, n_jobs=-1)\n",
    "        model_name = \"xgboost-3\"\n",
    "\n",
    "        print(f\"{target_horizon} | {pi_handling} | {list_name} | {train_year} | {pi_year} | {min_feat} | {days_assessed}\")\n",
    "        results_df = deployment_flow.run_deploy_flow(days_assessed, r, pi_handling, feature_cols, df_daily, model_name, model,\n",
    "                        train_year, pi_year, min_feat, list_name, feature_dict, groups, type)\n",
    "        \n",
    "        master_results.append(results_df)\n",
    "\n",
    "master_results_df = pd.concat(master_results, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Join in Performance\n",
    "- Need to add in close from prediction and current level and if in the money"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pprec</th>\n",
       "      <th>nprec</th>\n",
       "      <th>horizon</th>\n",
       "      <th>features</th>\n",
       "      <th>train_years</th>\n",
       "      <th>min_feats</th>\n",
       "      <th>pi_size</th>\n",
       "      <th>pi_handling</th>\n",
       "      <th>model</th>\n",
       "      <th>run</th>\n",
       "      <th>test_days</th>\n",
       "      <th>pred</th>\n",
       "      <th>train_n</th>\n",
       "      <th>train_start</th>\n",
       "      <th>train_end</th>\n",
       "      <th>test_start</th>\n",
       "      <th>test_end</th>\n",
       "      <th>n_features</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.64</td>\n",
       "      <td>0.53</td>\n",
       "      <td>2</td>\n",
       "      <td>volu-past_return</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>1.5</td>\n",
       "      <td>run_separately</td>\n",
       "      <td>xgboost-3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.95</td>\n",
       "      <td>735</td>\n",
       "      <td>2023-03-08</td>\n",
       "      <td>2026-02-10</td>\n",
       "      <td>2026-02-13</td>\n",
       "      <td>2026-02-13</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.64</td>\n",
       "      <td>0.53</td>\n",
       "      <td>2</td>\n",
       "      <td>volu-past_return</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>1.5</td>\n",
       "      <td>run_separately</td>\n",
       "      <td>xgboost-3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0.75</td>\n",
       "      <td>735</td>\n",
       "      <td>2023-03-07</td>\n",
       "      <td>2026-02-09</td>\n",
       "      <td>2026-02-12</td>\n",
       "      <td>2026-02-12</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.62</td>\n",
       "      <td>0.54</td>\n",
       "      <td>2</td>\n",
       "      <td>ma_sma-volu</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>1.5</td>\n",
       "      <td>run_separately</td>\n",
       "      <td>xgboost-3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.20</td>\n",
       "      <td>735</td>\n",
       "      <td>2023-03-08</td>\n",
       "      <td>2026-02-10</td>\n",
       "      <td>2026-02-13</td>\n",
       "      <td>2026-02-13</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.62</td>\n",
       "      <td>0.54</td>\n",
       "      <td>2</td>\n",
       "      <td>ma_sma-volu</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>1.5</td>\n",
       "      <td>run_separately</td>\n",
       "      <td>xgboost-3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0.05</td>\n",
       "      <td>735</td>\n",
       "      <td>2023-03-07</td>\n",
       "      <td>2026-02-09</td>\n",
       "      <td>2026-02-12</td>\n",
       "      <td>2026-02-12</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.64</td>\n",
       "      <td>0.50</td>\n",
       "      <td>2</td>\n",
       "      <td>ma_num-vix_skew</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>1.5</td>\n",
       "      <td>run_separately</td>\n",
       "      <td>xgboost-3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.70</td>\n",
       "      <td>735</td>\n",
       "      <td>2023-03-08</td>\n",
       "      <td>2026-02-10</td>\n",
       "      <td>2026-02-13</td>\n",
       "      <td>2026-02-13</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.64</td>\n",
       "      <td>0.50</td>\n",
       "      <td>2</td>\n",
       "      <td>ma_num-vix_skew</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>1.5</td>\n",
       "      <td>run_separately</td>\n",
       "      <td>xgboost-3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0.90</td>\n",
       "      <td>735</td>\n",
       "      <td>2023-03-07</td>\n",
       "      <td>2026-02-09</td>\n",
       "      <td>2026-02-12</td>\n",
       "      <td>2026-02-12</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.61</td>\n",
       "      <td>0.49</td>\n",
       "      <td>2</td>\n",
       "      <td>ma_lag-vix_skew</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>1.5</td>\n",
       "      <td>run_separately</td>\n",
       "      <td>xgboost-3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.50</td>\n",
       "      <td>735</td>\n",
       "      <td>2023-03-08</td>\n",
       "      <td>2026-02-10</td>\n",
       "      <td>2026-02-13</td>\n",
       "      <td>2026-02-13</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.61</td>\n",
       "      <td>0.49</td>\n",
       "      <td>2</td>\n",
       "      <td>ma_lag-vix_skew</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>1.5</td>\n",
       "      <td>run_separately</td>\n",
       "      <td>xgboost-3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0.90</td>\n",
       "      <td>735</td>\n",
       "      <td>2023-03-07</td>\n",
       "      <td>2026-02-09</td>\n",
       "      <td>2026-02-12</td>\n",
       "      <td>2026-02-12</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.57</td>\n",
       "      <td>0.54</td>\n",
       "      <td>2</td>\n",
       "      <td>atr_adxvola-past_return</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>1.5</td>\n",
       "      <td>run_separately</td>\n",
       "      <td>xgboost-3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.90</td>\n",
       "      <td>735</td>\n",
       "      <td>2023-03-08</td>\n",
       "      <td>2026-02-10</td>\n",
       "      <td>2026-02-13</td>\n",
       "      <td>2026-02-13</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.57</td>\n",
       "      <td>0.54</td>\n",
       "      <td>2</td>\n",
       "      <td>atr_adxvola-past_return</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>1.5</td>\n",
       "      <td>run_separately</td>\n",
       "      <td>xgboost-3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0.95</td>\n",
       "      <td>735</td>\n",
       "      <td>2023-03-07</td>\n",
       "      <td>2026-02-09</td>\n",
       "      <td>2026-02-12</td>\n",
       "      <td>2026-02-12</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   pprec  nprec  horizon                 features  train_years  min_feats  \\\n",
       "0   0.64   0.53        2         volu-past_return            3          4   \n",
       "1   0.64   0.53        2         volu-past_return            3          4   \n",
       "2   0.62   0.54        2              ma_sma-volu            3          4   \n",
       "3   0.62   0.54        2              ma_sma-volu            3          4   \n",
       "4   0.64   0.50        2          ma_num-vix_skew            3          4   \n",
       "5   0.64   0.50        2          ma_num-vix_skew            3          4   \n",
       "6   0.61   0.49        2          ma_lag-vix_skew            3          4   \n",
       "7   0.61   0.49        2          ma_lag-vix_skew            3          4   \n",
       "8   0.57   0.54        2  atr_adxvola-past_return            3          4   \n",
       "9   0.57   0.54        2  atr_adxvola-past_return            3          4   \n",
       "\n",
       "   pi_size     pi_handling      model  run  test_days  pred  train_n  \\\n",
       "0      1.5  run_separately  xgboost-3    1          1  0.95      735   \n",
       "1      1.5  run_separately  xgboost-3    2          1  0.75      735   \n",
       "2      1.5  run_separately  xgboost-3    1          1  0.20      735   \n",
       "3      1.5  run_separately  xgboost-3    2          1  0.05      735   \n",
       "4      1.5  run_separately  xgboost-3    1          1  0.70      735   \n",
       "5      1.5  run_separately  xgboost-3    2          1  0.90      735   \n",
       "6      1.5  run_separately  xgboost-3    1          1  0.50      735   \n",
       "7      1.5  run_separately  xgboost-3    2          1  0.90      735   \n",
       "8      1.5  run_separately  xgboost-3    1          1  0.90      735   \n",
       "9      1.5  run_separately  xgboost-3    2          1  0.95      735   \n",
       "\n",
       "  train_start   train_end  test_start    test_end  n_features  \n",
       "0  2023-03-08  2026-02-10  2026-02-13  2026-02-13          10  \n",
       "1  2023-03-07  2026-02-09  2026-02-12  2026-02-12          10  \n",
       "2  2023-03-08  2026-02-10  2026-02-13  2026-02-13          15  \n",
       "3  2023-03-07  2026-02-09  2026-02-12  2026-02-12          15  \n",
       "4  2023-03-08  2026-02-10  2026-02-13  2026-02-13          13  \n",
       "5  2023-03-07  2026-02-09  2026-02-12  2026-02-12          13  \n",
       "6  2023-03-08  2026-02-10  2026-02-13  2026-02-13          10  \n",
       "7  2023-03-07  2026-02-09  2026-02-12  2026-02-12          10  \n",
       "8  2023-03-08  2026-02-10  2026-02-13  2026-02-13          13  \n",
       "9  2023-03-07  2026-02-09  2026-02-12  2026-02-12          12  "
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions_df = master_results_df.copy()\n",
    "top_10_r[['pprec', 'nprec'] + keys].drop_duplicates().merge(predictions_df, on=keys, how=\"inner\")\n",
    "#predictions_df.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Archived"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_csvs():\n",
    "    \n",
    "    for r in returns:\n",
    "        \n",
    "        past_performance = pd.read_csv(f\"h{r}_baseline.csv\") # Match prior cell saved as file name horizon_2_baseline_new\n",
    "        \"\"\"\n",
    "        past_performance[\"features\"] = (\n",
    "        past_performance[\"feature_set\"]\n",
    "            .str.replace(\"_ba$\", \"\", regex=True)\n",
    "            .str.replace(\"-baseline$\", \"\", regex=True)\n",
    "            .str.replace(\"-pr$\", \"-past_return\", regex=True)\n",
    "            .str.replace(\"past_ret_cols\", \"past_return\", regex=False)\n",
    "        )\n",
    "        \"\"\"\n",
    "\n",
    "        past_performance[\"pi_handling\"] = (\n",
    "        past_performance[\"pi_size\"]\n",
    "        .str.replace(\"1.5-r\", \"run_separately\", regex=False)\n",
    "        .str.replace(\"1.5\", \"include_new\", regex=False)\n",
    "        )\n",
    "\n",
    "        past_performance[\"pi_size\"] = 1.5\n",
    "        past_performance.to_csv(f\"h{r}_baseline.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
